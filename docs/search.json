[{"path":"/articles/S1_probability.html","id":"learning-objectives","dir":"Articles","previous_headings":"","what":"Learning objectives","title":"S1_probability: Definitions and exercises","text":"understand use sample() understand use set.seed() control behavior random number generation distinguish sample size experiment replication size simulation Explain relationship precision probability estimate sample size associated experiment binomially distributed outcomes outcomes governed Poisson Negative Binomial models general Understand simulate data null (e.g., fair deck) alternative (e.g., biased deck) conditions Understand difference finite, countably infinite, uncountably infinite sample spaces Define use probability density functions cumulative distribution functions continuous outcomes joint probability distribution two random quantities contours bivariate density: empirical model-based covariance correlation","code":""},{"path":"/articles/S1_probability.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"S1_probability: Definitions and exercises","text":"concept probability “long run relative frequency”. ’ll work several models random events make concrete.","code":""},{"path":[]},{"path":[]},{"path":"/articles/S1_probability.html","id":"random-permutations-with-sample","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event > Simulating shuffled playing cards","what":"Random permutations with sample()","title":"S1_probability: Definitions and exercises","text":"’ll take granted sample(x, size=length(x), replace=FALSE) R achieves goal “shuffling” elements x. Thus assume , x vector R, aspect ordering elements s1 can used predict anything ordering elements s2. words, sample(x, size, replace=FALSE) taken primitive operation permutes elements x “random” way.","code":"s1 = sample(x, size=length(x), replace=FALSE) s2 = sample(x, size=length(x), replace=FALSE) set.seed(5432) # initialize, for reproducibility, the random number generator sample(1:5, replace=FALSE) # permute (1,2,3,4,5) ## [1] 4 2 1 3 5 sample(1:5, replace=FALSE) # a new, unpredictable, permutation ## [1] 3 4 1 2 5"},{"path":"/articles/S1_probability.html","id":"exercises","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event > Simulating shuffled playing cards > Random permutations with sample()","what":"Exercises","title":"S1_probability: Definitions and exercises","text":"1: second permutation produced termed “unpredictable”? 2: Predict outcome sample(1:5, replace=FALSE), run just one shown .","code":""},{"path":"/articles/S1_probability.html","id":"card-deck-definitions-and-operations","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event > Simulating shuffled playing cards","what":"Card deck definitions and operations","title":"S1_probability: Definitions and exercises","text":"card deck vector 52 elements. Unicode characters encode “suits”. already installed CSHstats (long time ago) use get access software needed computations document. fair deck one card combination “face” “suit”.","code":"BiocManager::install(\"vjcitn/CSHstats\")  # install.packages(\"BiocManager\") if necessary library(CSHstats) d = build_deck() d ##  [1] \"2 ♡\"  \"3 ♡\"  \"4 ♡\"  \"5 ♡\"  \"6 ♡\"  \"7 ♡\"  \"8 ♡\"  \"9 ♡\"  \"10 ♡\" \"J ♡\"  ## [11] \"Q ♡\"  \"K ♡\"  \"A ♡\"  \"2 ♢\"  \"3 ♢\"  \"4 ♢\"  \"5 ♢\"  \"6 ♢\"  \"7 ♢\"  \"8 ♢\"  ## [21] \"9 ♢\"  \"10 ♢\" \"J ♢\"  \"Q ♢\"  \"K ♢\"  \"A ♢\"  \"2 ♣\"  \"3 ♣\"  \"4 ♣\"  \"5 ♣\"  ## [31] \"6 ♣\"  \"7 ♣\"  \"8 ♣\"  \"9 ♣\"  \"10 ♣\" \"J ♣\"  \"Q ♣\"  \"K ♣\"  \"A ♣\"  \"2 ♤\"  ## [41] \"3 ♤\"  \"4 ♤\"  \"5 ♤\"  \"6 ♤\"  \"7 ♤\"  \"8 ♤\"  \"9 ♤\"  \"10 ♤\" \"J ♤\"  \"Q ♤\"  ## [51] \"K ♤\"  \"A ♤\" unique(suits(d)) ## [1] \"♡\" \"♢\" \"♣\" \"♤\" unique(faces(d)) ##  [1] \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"J\"  \"Q\"  \"K\"  \"A\" table(suits(d), faces(d)) ##     ##     10 2 3 4 5 6 7 8 9 A J K Q ##   ♡  1 1 1 1 1 1 1 1 1 1 1 1 1 ##   ♢  1 1 1 1 1 1 1 1 1 1 1 1 1 ##   ♣  1 1 1 1 1 1 1 1 1 1 1 1 1 ##   ♤  1 1 1 1 1 1 1 1 1 1 1 1 1"},{"path":"/articles/S1_probability.html","id":"exercises-1","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event > Simulating shuffled playing cards > Card deck definitions and operations","what":"Exercises","title":"S1_probability: Definitions and exercises","text":"3: Write code produces new deck fair except two copies ten hearts. Verify new deck property.","code":""},{"path":"/articles/S1_probability.html","id":"shuffling-the-deck","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event > Simulating shuffled playing cards","what":"Shuffling the deck","title":"S1_probability: Definitions and exercises","text":"reproducible shuffling event can programmed follows:","code":"set.seed(1234)  # any numeric seed will do but you need to record it shuffle_deck = function(d) sample(d, size=length(d), replace=FALSE) head(d)         # top 6 cards ## [1] \"2 ♡\" \"3 ♡\" \"4 ♡\" \"5 ♡\" \"6 ♡\" \"7 ♡\" head(shuffle_deck(d)) ## [1] \"3 ♣\"  \"4 ♢\"  \"10 ♢\" \"Q ♣\"  \"6 ♤\"  \"9 ♤\""},{"path":"/articles/S1_probability.html","id":"the-top-card-after-a-shuffle","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event > Simulating shuffled playing cards","what":"The top card after a shuffle","title":"S1_probability: Definitions and exercises","text":"","code":"set.seed(4141) top_draw = function(d) shuffle_deck(d)[1] top_draw(d) ## [1] \"A ♤\" top_draw(d) ## [1] \"5 ♤\""},{"path":"/articles/S1_probability.html","id":"exercises-2","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event > Simulating shuffled playing cards > The top card after a shuffle","what":"Exercises","title":"S1_probability: Definitions and exercises","text":"4: top card next shuffle? 5: Write code get third card. 6: probability top draw shuffled fair deck heart?","code":""},{"path":[]},{"path":"/articles/S1_probability.html","id":"a-simulation-process-repeated-shuffles-and-draws","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event > Estimating the probability of an event","what":"A simulation process: repeated shuffles and draws","title":"S1_probability: Definitions and exercises","text":"’s simple way estimating probability top draw shuffled fair deck heart, construed long run frequency. ’ll simulate 100 shuffles save results testing suit top card. concise approach R :","code":"heart_sign = function() \"\\U2661\"  # unicode U2661, prepend backslash heart_sign() ## [1] \"♡\" res = rep(NA, 100) for (i in 1:100) {  res[i] = (suits(top_draw(d)) == heart_sign()) # TRUE if top card is a heart } head(res) ## [1] FALSE FALSE  TRUE  TRUE  TRUE FALSE ### relative frequency of the event \"suit of top card is 'heart'\" sum(res)/length(res) ## [1] 0.36 mean(replicate( 100, suits(top_draw(d)) == heart_sign()) ) ## [1] 0.22"},{"path":"/articles/S1_probability.html","id":"exercises-3","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event > Estimating the probability of an event > A simulation process: repeated shuffles and draws","what":"Exercises","title":"S1_probability: Definitions and exercises","text":"7: interpret difference two probability estimates given ?","code":""},{"path":"/articles/S1_probability.html","id":"replicating-the-estimation-process","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event > Estimating the probability of an event","what":"Replicating the estimation process","title":"S1_probability: Definitions and exercises","text":"Let’s intensify investigation aim understanding uncertainty estimation. ’ll define variable gives size “experiment”: shuffling 100 times ’ll refer sample size, SSIZE study estimation procedure replicating experiment N_REPLICATION times.","code":"SSIZE = 100 N_REPLICATION = 500 set.seed(10101) # initialize, for reproducibility, the random number generator doubsim = replicate(N_REPLICATION,      mean(replicate( SSIZE, suits(top_draw(d)) == heart_sign()) )     ) hist(doubsim, xlim=c(0,1))"},{"path":"/articles/S1_probability.html","id":"exercises-4","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event > Estimating the probability of an event > Replicating the estimation process","what":"Exercises","title":"S1_probability: Definitions and exercises","text":"8: can make estimate probability event “suit top card ‘heart’” precise? Answer: increase SSIZE.  increased “sample size”, reduced variation experiment--experiment estimates probability heart suit top card.","code":"SSIZE = 500 set.seed(101012) doubsim2 = replicate(N_REPLICATION,      mean(replicate( SSIZE, suits(top_draw(d)) == heart_sign()) )     ) hist(doubsim2, xlim=c(0,1))"},{"path":"/articles/S1_probability.html","id":"formal-probability-models","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event","what":"Formal probability models","title":"S1_probability: Definitions and exercises","text":"Intuitively, probability drawing heart well-shuffled fair deck 1/4. repeat shuffling drawing one hundred times, expect around 25 draws reveal heart. Formal probability models enable us reason systematically mean around description expectation. models can also create accurate predictions likely outcomes complex events. useful define sample space probability model, set possible outcomes random process modeled. top-card-drawn example, sample space set suits, disregard face value. fair deck, point sample space probability \\(1/4\\). Events interest “top card heart” “top card heart”. probabilities events derivable probabilities constituent outcomes.","code":""},{"path":"/articles/S1_probability.html","id":"using-the-binomial-probability-model","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event > Formal probability models","what":"Using the binomial probability model","title":"S1_probability: Definitions and exercises","text":"series independent dichotomous events (true false, zero one, heart non-heart) can modeled using probability mass function binomial distribution. two parameters, \\(p\\) \\(n\\), \\(p\\) (unobservable) probability event (say “suit top card ‘heart’”) \\(n\\) number independent trials random dichotomy observed. \\(n\\) “trials”, event probability \\(p\\), probability seeing event \\(x\\) times \\[ Pr(X = x; n, p) = {\\binom{n}{x}} p^x(1-p)^{n-x} \\] written \\(X\\) denote random quantity \\(x\\) denote realization. single draw, \\(X\\) count hearts seen draw, \\[ Pr(X=0; 1, 1/4) = 1-1/4 = 3/4 \\] \\[ Pr(X=1; 1, 1/4) = 1/4 \\]","code":""},{"path":"/articles/S1_probability.html","id":"exercises-5","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event > Formal probability models > Using the binomial probability model","what":"Exercises","title":"S1_probability: Definitions and exercises","text":"9: Modify production doubsim2 \\(x\\)-axis histogram units “number times top card heart”.","code":"hist(doubsim3, xlim=c(.15*500,.35*500), xlab=\"count of draws with heart as suit of top card\")"},{"path":"/articles/S1_probability.html","id":"visualizing-the-model-and-the-data","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event > Formal probability models","what":"Visualizing the model and the data","title":"S1_probability: Definitions and exercises","text":"formula given tells us frequently observe given count. R function dbinom can compute probability, multiply number realizations get height histogram.  Notice histogram, virtue binning counts, seem reflect shape theoretical frequency function given dots. can remedied increasing number replicates used. considerable research regarding design histogram displays. See ?nclass.Sturges references. One unpleasant feature display doubsim2 seems imply asymmetric distribution. Another way “cuts ” extremes.","code":"hist(doubsim3, xlim=c(.15*500,.35*500),     xlab=\"count of draws with heart as suit of top card\", ylim=c(0,115)) points(80:160, 2500*dbinom(80:160, 500, 13/52), pch=19, cex=.5) legend(78, 110, pch=19, legend=\"scaled dbinom(x, 500, .25)\", bty=\"n\", cex=.85)"},{"path":"/articles/S1_probability.html","id":"exercises-6","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event > Formal probability models > Visualizing the model and the data","what":"Exercises","title":"S1_probability: Definitions and exercises","text":"10: Write couple sentences carefully distinguishing displays doubsim2 doubsim3 . Explain choice heuristic name “doubsim”.","code":""},{"path":"/articles/S1_probability.html","id":"a-biased-deck","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event","what":"A biased deck","title":"S1_probability: Definitions and exercises","text":"Recall layout fair deck: make copy one card remove one:","code":"table(suits(d), faces(d)) ##     ##     10 2 3 4 5 6 7 8 9 A J K Q ##   ♡  1 1 1 1 1 1 1 1 1 1 1 1 1 ##   ♢  1 1 1 1 1 1 1 1 1 1 1 1 1 ##   ♣  1 1 1 1 1 1 1 1 1 1 1 1 1 ##   ♤  1 1 1 1 1 1 1 1 1 1 1 1 1 bd = d bd[18] = bd[3] table(suits(bd), faces(bd)) ##     ##     10 2 3 4 5 6 7 8 9 A J K Q ##   ♡  1 1 1 2 1 1 1 1 1 1 1 1 1 ##   ♢  1 1 1 1 1 0 1 1 1 1 1 1 1 ##   ♣  1 1 1 1 1 1 1 1 1 1 1 1 1 ##   ♤  1 1 1 1 1 1 1 1 1 1 1 1 1"},{"path":"/articles/S1_probability.html","id":"exercises-7","dir":"Articles","previous_headings":"The probability of a 0-1 (binary or dichotomous) event > A biased deck","what":"Exercises","title":"S1_probability: Definitions and exercises","text":"11: probability top card drawn shuffle bd heart? 12: probability top card drawn shuffle bd club?","code":""},{"path":"/articles/S1_probability.html","id":"probability-models-for-categorical-outcomes","dir":"Articles","previous_headings":"","what":"Probability models for categorical outcomes","title":"S1_probability: Definitions and exercises","text":"Thus far focused dichotomy: top draw heart . can consider possible suits 4-valued response.","code":""},{"path":"/articles/S1_probability.html","id":"a-contingency-table","dir":"Articles","previous_headings":"Probability models for categorical outcomes","what":"A contingency table","title":"S1_probability: Definitions and exercises","text":"Let’s simulate process drawing top card shuffling, tabulate suits observed.","code":"set.seed(4321) table( tops <- replicate(500, suits(top_draw(bd))) ) ##  ##   ♡   ♢   ♣   ♤  ## 148 106 126 120"},{"path":"/articles/S1_probability.html","id":"exercises-8","dir":"Articles","previous_headings":"Probability models for categorical outcomes > A contingency table","what":"Exercises","title":"S1_probability: Definitions and exercises","text":"13: expect counts suits table deck fair?","code":""},{"path":"/articles/S1_probability.html","id":"multinomial-model-and-simulation","dir":"Articles","previous_headings":"Probability models for categorical outcomes","what":"Multinomial model and simulation","title":"S1_probability: Definitions and exercises","text":"adopted binomial model number top draws suit “heart” fixed number shuffles: \\[ Pr(X = x; n, p) = {\\binom{n}{x}} p^x(1-p)^{n-x} \\] fair deck, \\(p= 1/4\\). generalization vector responses multinomial model. can use (ordered) vector counts top draws yielding different suits. problem parameters \\(N\\) (number trials), \\(k\\) (number categories), \\(p_1, \\ldots, p_k\\), category-specific probabilities. realizations denoted \\(x_1, \\ldots, x_k\\) \\(\\sum_i x_i = N\\). Now probability model defined terms random vectors vectors probabilities: \\[ Pr(X_1 = x_1, \\ldots, X_k = x_k) = \\frac{n!}{x_1! \\cdots x_k!} p_1^{x_1} \\cdots p_k^{x_k} \\] provides elegant way producing frequency distributions suit top draw. introduce pseudorandom number generation multinomial model, using rmultinom. Notice use deck d producing matrix counts. Previously applied sample() 52-vector cards. Now use model develop data interest.","code":"NREP = 10000 SSIZE = 500 mnmat = rmultinom(NREP, SSIZE, rep(.25,4)) rownames(mnmat) = c(\"\\U2661\", \"\\U2662\", \"\\U2663\", \"\\U2664\") mnmat[,1]  # one draw ##   ♡   ♢   ♣   ♤  ## 131 125 110 134 apply(mnmat,1,mean) ##        ♡        ♢        ♣        ♤  ## 124.9621 124.9080 125.0362 125.0937"},{"path":"/articles/S1_probability.html","id":"exercise","dir":"Articles","previous_headings":"Probability models for categorical outcomes > Multinomial model and simulation","what":"Exercise","title":"S1_probability: Definitions and exercises","text":"14: Modify call rmultinom obtain distributions top card suits biased deck bd. Hint: Change part call involving rep().","code":""},{"path":[]},{"path":"/articles/S1_probability.html","id":"modeling-collections-of-counts","dir":"Articles","previous_headings":"Probability models for categorical outcomes > The Poisson distribution","what":"Modeling collections of counts","title":"S1_probability: Definitions and exercises","text":"Outcomes taking form integer counts arise many appications. example, quarry floor divided squares one meter side. 30 squares searched fossil number specimens per square recorded. square 5 specimens. (Example Probability Models Applications, . Olkin, L. Gleser, C. Derman, ch 6.3.) table includes nspec, number specimens square, freq, number squares corresponding number specimens, pred, predicted number specimens based Poisson model mean parameter 0.73 specimens per square. derive quantities pred, used dpois counts per square, mean parameter set 0.73. derive , use obtain average number fossils per square. Poisson model mean \\(\\lambda\\) \\[ Pr(X = k; \\lambda) = \\frac{e^{-\\lambda}\\lambda^k}{k!} \\]","code":"foss = data.frame(nspec=0:4, freq=c(16,9,3,1,1), pred=round(30*dpois(0:4, 0.73), 2)) foss ##   nspec freq  pred ## 1     0   16 14.46 ## 2     1    9 10.55 ## 3     2    3  3.85 ## 4     3    1  0.94 ## 5     4    1  0.17 sum(foss$nspec * foss$freq)/30 ## [1] 0.7333333"},{"path":"/articles/S1_probability.html","id":"exercise-1","dir":"Articles","previous_headings":"Probability models for categorical outcomes > The Poisson distribution","what":"Exercise","title":"S1_probability: Definitions and exercises","text":"15: Use formula explain prediction 14.46 squares zero fossils.","code":""},{"path":"/articles/S1_probability.html","id":"a-countably-infinite-sample-space","dir":"Articles","previous_headings":"Probability models for categorical outcomes > The Poisson distribution","what":"A countably infinite sample space","title":"S1_probability: Definitions and exercises","text":"binomial multinomial models discussed finite discrete sample spaces. sample space Poisson model set non-negative integers. mass function arises fact \\(e^\\lambda = \\sum \\lambda^k/k!\\), sum taken non-negative integers.","code":""},{"path":"/articles/S1_probability.html","id":"the-negative-binomial-model","dir":"Articles","previous_headings":"Probability models for categorical outcomes","what":"The negative binomial model","title":"S1_probability: Definitions and exercises","text":"mass function \\[ p(x; \\theta, \\mu) = \\frac{\\Gamma(\\theta + x)}{\\Gamma(\\theta)y!}     \\frac{\\mu^y \\theta^\\theta}{(\\mu + \\theta)^{\\theta + y}} \\] useful later material.","code":""},{"path":"/articles/S1_probability.html","id":"mean-variance","dir":"Articles","previous_headings":"Probability models for categorical outcomes","what":"Mean, variance","title":"S1_probability: Definitions and exercises","text":"sample mean vector \\(x = (x_1, \\ldots, x_n)\\) defined \\(\\bar{x} = n^{-1}\\sum_i x_i\\). (unbiased estimator ) sample variance \\((n-1)^{-1}\\sum_i (x_i - \\bar{x})^2\\). sometimes refer mean variance context probability models rather samples. mean continuous distribution defined discrete distribution probability mass function \\(p\\) mean \\[ E(x) = \\sum x p(x), \\] sum taken sample space associated model. Writing \\(\\mu\\) mean value distribution study, variance discrete distribution \\[ V(x) = \\sum (x-\\mu)^2 p(x) dx. \\]","code":""},{"path":"/articles/S1_probability.html","id":"models-for-continuous-outcomes","dir":"Articles","previous_headings":"","what":"Models for continuous outcomes","title":"S1_probability: Definitions and exercises","text":"models ’ve considered thus far discrete responses – sample space either finite countably infinite. continuous responses, sample space uncountably infinite. sample space may interval real line, whole real line. discuss define probabilities subsets sample space.","code":""},{"path":"/articles/S1_probability.html","id":"univariate-response","dir":"Articles","previous_headings":"Models for continuous outcomes","what":"Univariate response","title":"S1_probability: Definitions and exercises","text":"concreteness, use RNA-seq data derived Cancer Genome Atlas. ’ll focus 79 values normalized RSEM-based estimates expression YY1 adrenocortical carcinoma. refer vector “real valued” outcomes.","code":"data(yy1_ex) head(yy1_ex) ## TCGA-OR-A5J1-01A-11R-A29S-07 TCGA-OR-A5J2-01A-11R-A29S-07  ##                    1455.8117                    1688.6318  ## TCGA-OR-A5J3-01A-11R-A29S-07 TCGA-OR-A5J5-01A-11R-A29S-07  ##                    1821.3598                    1877.6143  ## TCGA-OR-A5J6-01A-31R-A29S-07 TCGA-OR-A5J7-01A-11R-A29S-07  ##                     634.3023                    1132.3706"},{"path":"/articles/S1_probability.html","id":"cumulative-distribution-function-quantiles","dir":"Articles","previous_headings":"Models for continuous outcomes > Univariate response","what":"Cumulative distribution function; quantiles","title":"S1_probability: Definitions and exercises","text":"’ll use capital letters denote random variables. formal definition “random variable” given textbooks; use term informally refer quantities regard random. function \\(F(x) = \\mbox{Pr}(X < x)\\) called cumulative distribution function (CDF) random variable \\(X\\). YY1 measures, can produce visualization associated CDF:  quantiles distribution given inverse CDF. quantile function takes number \\(p\\) 0 1 returns number \\(F^{-1}(p)\\). graph shows 0.80 quantile distribution YY1 vector 1584.","code":"plot(ecdf(yy1_ex)) abline(h=.8, lty=2) arrows(1583, .8, 1583, 0, lty=2)"},{"path":"/articles/S1_probability.html","id":"exercise-2","dir":"Articles","previous_headings":"Models for continuous outcomes > Univariate response","what":"Exercise","title":"S1_probability: Definitions and exercises","text":"16: Show proportion YY1 observations exceeding 1584 approximately 20%.","code":""},{"path":"/articles/S1_probability.html","id":"histogram-probability-density-function","dir":"Articles","previous_headings":"Models for continuous outcomes > Univariate response","what":"Histogram, probability density function","title":"S1_probability: Definitions and exercises","text":"histogram tunable display relative frequencies values vector.  probability density function continuous random variable \\(X\\) satisfies \\[ \\int_a^b f(x) dx = \\mbox{Pr}(< X < b) \\] can estimate density function sample various ways. simple illustration:  interest use approxfun numerical integration density estimate given , estimate probability YY1 expression lies given interval. acceptable? changing bandwidth density estimator produce better results? potential overfitting problem?","code":"par(mfrow=c(2,1), mar=c(4,3,1,1)) hist(yy1_ex, xlim=c(400, 2600)) hist(yy1_ex, breaks=20, xlim=c(400, 2600)) plot(dd <- density(yy1_ex)) dfun = approxfun(dd$x, dd$y) integrate(dfun, 1000, 1500)  # density-based ## 0.4821651 with absolute error < 1e-05 mean(yy1_ex >=1000 & yy1_ex <= 1500) # empirical ## [1] 0.5316456"},{"path":"/articles/S1_probability.html","id":"some-widely-used-models-for-continuous-responses","dir":"Articles","previous_headings":"Models for continuous outcomes > Univariate response","what":"Some widely used models for continuous responses","title":"S1_probability: Definitions and exercises","text":"’ll use simulation illustrate shapes various distributions. R makes simple family functions whose names begin r.","code":""},{"path":"/articles/S1_probability.html","id":"uniform-distribution-on-01","dir":"Articles","previous_headings":"Models for continuous outcomes > Univariate response > Some widely used models for continuous responses","what":"Uniform distribution on [0,1]","title":"S1_probability: Definitions and exercises","text":"density function \\(f(x) = 1\\) \\(x \\[0,1]\\) 0 otherwise.","code":"hist(runif(10000, 0, 1), prob=TRUE)"},{"path":"/articles/S1_probability.html","id":"exercise-3","dir":"Articles","previous_headings":"","what":"S1_probability: Definitions and exercises","title":"S1_probability: Definitions and exercises","text":"17: Use code like obtain value density random variable uniform density interval [0, 2].","code":""},{"path":"/articles/S1_probability.html","id":"gaussian-model","dir":"Articles","previous_headings":"Models for continuous outcomes > Univariate response > Some widely used models for continuous responses","what":"Gaussian model","title":"S1_probability: Definitions and exercises","text":"Gaussian distribution also called “normal”. shape position real line determined mean variance. Symbolically, model often written \\(N(\\mu, \\sigma^2)\\), density function \\(f(x) = 1/\\sqrt{2\\pi \\sigma^2} \\exp\\{(x-\\mu)^2/2 \\sigma^2\\}\\).","code":"hist(rnorm(10000, 0, 1), prob=TRUE) lines(seq(-4,4,len=100), dnorm(seq(-4,4,len=100)))"},{"path":"/articles/S1_probability.html","id":"exponential-and-gamma-models-for-survival-data","dir":"Articles","previous_headings":"Models for continuous outcomes > Univariate response > Some widely used models for continuous responses","what":"Exponential and Gamma models for survival data","title":"S1_probability: Definitions and exercises","text":"exponential model (confused exponential family models) defined random variables positive values. example survival times. ’ll use dataset survival package illustrate.  density function exponential model \\(f(x) = \\lambda e ^ {-\\lambda x}\\). formulation, \\(\\lambda\\) referred rate parameter. mean distribution \\(1/\\lambda\\).","code":"library(survival) # use myeloma records from Mayo clinic, remove censored times md = myeloma$futime[myeloma$death==1 & myeloma$entry == 0] hist(md, breaks=20, prob=TRUE, ylim=c(0,.001)) mean(md) ## [1] 1044.768 ss = seq(0,8000,.1) lines(ss, dexp(ss, rate=1/mean(md)))"},{"path":"/articles/S1_probability.html","id":"exercise-4","dir":"Articles","previous_headings":"","what":"S1_probability: Definitions and exercises","title":"S1_probability: Definitions and exercises","text":"18: close median exponential model rate 1/1045 sample median myeloma survival times? relate interpretation plot ?","code":""},{"path":"/articles/S1_probability.html","id":"mean-and-variance-for-continuous-models","dir":"Articles","previous_headings":"Models for continuous outcomes","what":"Mean and variance for continuous models","title":"S1_probability: Definitions and exercises","text":"continuous distributions, mean value \\[ E(x) = \\int x f(x) dx. \\] Writing \\(\\mu\\) mean value distribution study, variance continuous distribution \\[ V(x) = \\int (x-\\mu)^2 f(x) dx. \\]","code":""},{"path":"/articles/S1_probability.html","id":"multivariate-response","dir":"Articles","previous_headings":"Models for continuous outcomes","what":"Multivariate response","title":"S1_probability: Definitions and exercises","text":"’ve seen use univariate probability models many facets. concept joint distribution multiple random quantities central reasoning interactions among components complicated processes. following example, obtained normalized expression measures EGR1 FOS adrenocortical carcinoma tumors studied TCGA. ’ll use built-bivariate density estimation show relationship expression two transcription factors ACC.","code":"data(fos_ex) data(egr1_ex) bivdf = data.frame(fos_ex, egr1_ex) library(ggplot2) ggplot(bivdf, aes(x=log(fos_ex), y=log(egr1_ex))) + geom_point() + geom_density_2d()"},{"path":"/articles/S1_probability.html","id":"joint-cumulative-distribution-function","dir":"Articles","previous_headings":"Models for continuous outcomes > Multivariate response","what":"Joint cumulative distribution function","title":"S1_probability: Definitions and exercises","text":"’ll focus case continuous bivariate response denoted \\((X,Y)\\). joint cumulative distribution function (cdf) \\[ F(x,y) = Pr(X < x, Y < y) \\] , log FOS log EGR1, can evaluate \\(F(7,7)\\) follows: call empirical estimate \\(F(7,7)\\).","code":"mean(log(fos_ex)<7 & log(egr1_ex)<7) ## [1] 0.278481"},{"path":"/articles/S1_probability.html","id":"covariance-matrix-bivariate-case","dir":"Articles","previous_headings":"Models for continuous outcomes > Multivariate response","what":"Covariance matrix, bivariate case","title":"S1_probability: Definitions and exercises","text":"covariance two random variables \\(Cov(X,Y) = EXY - (EX)(EY)\\); covariance matrix symmetric, variances diagonal, covariance diagonal.","code":"var(log(fos_ex)) # univariate ## [1] 1.194019 covmat = var(cbind(log(fos_ex), log(egr1_ex))) # matrix covmat ##           [,1]      [,2] ## [1,] 1.1940188 0.9746936 ## [2,] 0.9746936 1.4137188"},{"path":"/articles/S1_probability.html","id":"using-the-bivariate-normal-model","dir":"Articles","previous_headings":"Models for continuous outcomes > Multivariate response","what":"Using the bivariate normal model","title":"S1_probability: Definitions and exercises","text":"Given estimates bivariate mean covariance, can use multivariate normal cumulative density function produce contours distribution elliptical shapes, opposed wiggly contours saw .  estimate \\(F(7,7)\\) using model ’ll call model-based estimate \\(F(7,7)\\).","code":"bmean = c(mean(log(fos_ex)), mean(log(egr1_ex))) library(mvtnorm) xgrid = seq(3,10,.05) ygrid = seq(3,10,.05) ngrid = length(xgrid) nd = matrix(NA, ngrid, ngrid) for (i in 1:ngrid) {  # inefficient!  for (j in 1:ngrid) {   nd[i,j] = dmvnorm(c(xgrid[i],ygrid[j]), bmean, covmat)   }  } contour(xgrid, ygrid, nd, ylab=\"log EGR1\", xlab=\"log FOS\") points(log(fos_ex), log(egr1_ex), col=\"gray\", pch=19) pmvnorm(upper=c(7,7), mean=bmean, sigma=covmat) ## [1] 0.3368775 ## attr(,\"error\") ## [1] 1e-15 ## attr(,\"msg\") ## [1] \"Normal Completion\""},{"path":"/articles/S1_probability.html","id":"exercise-5","dir":"Articles","previous_headings":"Models for continuous outcomes > Multivariate response > Using the bivariate normal model","what":"Exercise","title":"S1_probability: Definitions and exercises","text":"19: Obtain empirical model-based estimates \\(F(6,5)\\) (log FOS, log EGR1).","code":""},{"path":"/articles/S1_probability.html","id":"probabilistic-independence-and-dependence","dir":"Articles","previous_headings":"Models for continuous outcomes > Multivariate response","what":"Probabilistic independence and dependence","title":"S1_probability: Definitions and exercises","text":"“tilted contour ellipses” shown just indicate knowledge value log FOS tells us variation values log EGR1. log FOS 6, value log EGR1 much likely around 6 around 9. use concept conditional distribution address concept, denote conditional distribution function \\(F(x|y)\\), vertical bar denoting conditioning. Specifically, \\(F(x|Y=y)\\) = Pr(\\(X<x\\)) given \\(Y = y\\). Probabilistic independence two random quantities can formulated condition \\(F(x|y) = F(x)\\): knowledge value \\(Y\\) provides information distribution \\(X\\).","code":""},{"path":"/articles/S1_probability.html","id":"measures-of-correlation","dir":"Articles","previous_headings":"Models for continuous outcomes > Multivariate response","what":"Measures of correlation","title":"S1_probability: Definitions and exercises","text":"correlation coefficient two random quantities ratio covariance square root product variances. 20: Use elements covmat computed produce estimate correlation log FOS log EGR1, compare cor() R (log FOS, log EGR1).","code":""},{"path":"/articles/S2_EDA.html","id":"eda-exploratory-data-analysis","dir":"Articles","previous_headings":"","what":"EDA: Exploratory data analysis","title":"S2_EDA: Exploratory data analysis and visualization","text":"field EDA changed, thanks advances computational visualization technology. Classical EDA addressed Assessing distributional shape Comparing batches numbers: Boxplots beyond Transformations; handling “outliers” recently, can consider exploratory data activities use interactive tables interactive graphics purpose-built graphical user interfaces explore concerns. principles include high volumes involved, sampling dimension reduction relevant every facet data accessible exploration summarization","code":""},{"path":"/articles/S2_EDA.html","id":"learning-objectives","dir":"Articles","previous_headings":"","what":"Learning objectives","title":"S2_EDA: Exploratory data analysis and visualization","text":"simple statistics may fail disclose structure visualization always conducted understand might use shiny explore tunable data visualization Box-Cox transformation family provides clues choosing transformation look qqnorm, qqplot allows general comparisons Shiny + TnT allow exploration data context linear genome annotation","code":""},{"path":"/articles/S2_EDA.html","id":"anscombes-quartet-statistics-that-miss-the-point","dir":"Articles","previous_headings":"","what":"Anscombe’s quartet: statistics that miss the point","title":"S2_EDA: Exploratory data analysis and visualization","text":"1973 Francis Anscombe constructed four bivariate datasets. four datasets interest pairs columns (x1, y1), (x2, y2), …. discussion probability provided definitions mean, variance, covariance, correlation. quick check means variables quartet. use apply function general arguments X array-like entity, MARGIN defines dimension along FUN computed additional arguments possibly provided .... MARGIN 1, FUN computed “row”, MARGIN 2, FUN computed “column”. also introduce helper function rounded_stat rounds result univariate statistic. code demonstrates x y variables identical means standard deviations. Nevertheless, variables plotted one another, :","code":"library(DT) data(anscombe) datatable(anscombe) apply(X, MARGIN, FUN, ..., simplify=TRUE) rounded_stat = function(x, stat=mean, ndig=2) round(stat(x), ndig) apply(anscombe, 2, rounded_stat, stat=mean) ##  x1  x2  x3  x4  y1  y2  y3  y4  ## 9.0 9.0 9.0 9.0 7.5 7.5 7.5 7.5 apply(anscombe, 2, rounded_stat, stat=sd) ##   x1   x2   x3   x4   y1   y2   y3   y4  ## 3.32 3.32 3.32 3.32 2.03 2.03 2.03 2.03"},{"path":"/articles/S2_EDA.html","id":"exercises","dir":"Articles","previous_headings":"Anscombe’s quartet: statistics that miss the point","what":"Exercises","title":"S2_EDA: Exploratory data analysis and visualization","text":"1. Show four correlation coefficients (x1, y1), …, (x4, y4) identical (rounding). 2. blog post addresses tidyverse approach handling data. Read blog post try get comfortable various steps towards producing using “tidy” representation.","code":"# use BiocManager::install(\"tidyverse\") if necessary library(tidyverse) tidy_anscombe <- anscombe %>%  pivot_longer(cols = everything(),               names_to = c(\".value\", \"set\"),               names_pattern = \"(.)(.)\") library(ggplot2) ggplot(tidy_anscombe,        aes(x = x,            y = y)) +   geom_point() +    facet_wrap(~set) +   geom_smooth(method = \"lm\", se = FALSE) ## `geom_smooth()` using formula = 'y ~ x'"},{"path":[]},{"path":"/articles/S2_EDA.html","id":"univariate","dir":"Articles","previous_headings":"Exploring distributional shape","what":"Univariate","title":"S2_EDA: Exploratory data analysis and visualization","text":"mode distribution “common value”. peak histogram correspond mode corresponding distribution discrete random variable. continuous random variables, mode given peak density function. basic question univariate dataset : multimodal distribution? Live Exercise: programming interactive exploration. Use R studio’s File control produce new “Shiny app”, give name “modality”. use “Run app” control. slider controls number bins used summarize values times geyser eruption. many modes might underlying distribution ?","code":""},{"path":"/articles/S2_EDA.html","id":"transformation","dir":"Articles","previous_headings":"Exploring distributional shape","what":"Transformation","title":"S2_EDA: Exploratory data analysis and visualization","text":"search transformation best symmetrizes skewed distribution, boxcox function MASS library can used. origins procedure reported Royal Statistical Society 1964. JRSS paper basic idea can construct series functional transformations random variable produce distribution (transformed scale) approximately normal. Box-Cox family transformations indexed single parameter, \\(\\lambda\\). transformation variable \\(y\\) \\(y^*(\\lambda) = \\lambda^{-1}(y^\\lambda-1)\\) (\\(\\lambda \\neq 0\\)) log \\(y\\) \\(\\lambda = 0\\) provides smooth traversal (fractional) power, logarithm, reciprocal power operations \\(y\\). \\(\\lambda\\) 1, transformation indicated.  boxcox function MASS library trace likelihood function maximized distribution \\(y^*(\\lambda)\\) close Gaussian possible.","code":"opar = par(no.readonly=TRUE) par(mfrow=c(2,2), mar=c(3,2,1,1)) data(fos_ex) hist(fos_ex, main=\"raw\") hist(sqrt(fos_ex), main=\"sqrt\") hist(fos_ex^2, main=\"square\") hist(log(fos_ex), main=\"log\") par(opar) library(MASS) boxcox(fos_ex~1) # must use formula"},{"path":"/articles/S2_EDA.html","id":"exercise","dir":"Articles","previous_headings":"Exploring distributional shape > Transformation","what":"Exercise","title":"S2_EDA: Exploratory data analysis and visualization","text":"3. transformation boxcox() recommend applied log(egr1_ex)? 4. transformation boxcox() recommend applied geyser data started ?","code":""},{"path":"/articles/S2_EDA.html","id":"is-it-gaussian","dir":"Articles","previous_headings":"Exploring distributional shape","what":"Is it Gaussian?","title":"S2_EDA: Exploratory data analysis and visualization","text":"concept Q-Q plot (quantile-quantile plot) used compare given data vector reference distribution (distribution data vector).  configuration points Q-Q plot form straight line, reference distribution probably appropriate.","code":"qqnorm(fos_ex, main=\"Compare FOS to Gaussian distribution\")"},{"path":"/articles/S2_EDA.html","id":"exercises-1","dir":"Articles","previous_headings":"Exploring distributional shape > Is it Gaussian?","what":"Exercises","title":"S2_EDA: Exploratory data analysis and visualization","text":"5. Use qqnorm log(fos_ex) comment. 6. zsc = function(x) (x - mean(x))/sd(x). special qqnorm(zsc(log(fos_ex)))?","code":""},{"path":"/articles/S2_EDA.html","id":"bivariate","dir":"Articles","previous_headings":"Exploring distributional shape","what":"Bivariate","title":"S2_EDA: Exploratory data analysis and visualization","text":"can use density estimation plane reason multimodality bivariate data. example(geom_density_2d):","code":"library(datasets) data(faithful) m <- ggplot(faithful, aes(x = eruptions, y = waiting)) +   geom_point() + xlim(0.5, 6) + ylim(40, 110) # contour lines m + geom_density_2d()"},{"path":"/articles/S2_EDA.html","id":"exercise-1","dir":"Articles","previous_headings":"Exploring distributional shape > Bivariate","what":"Exercise","title":"S2_EDA: Exploratory data analysis and visualization","text":"7. Comment meaning “local modes” “marginal” displays.","code":"opar = par(no.readonly=TRUE) par(mfrow=c(1,2), mar=c(3,2,1,1)) plot(density(faithful$erup, .08)) plot(density(faithful$wait, 1.4)) par(opar)"},{"path":"/articles/S2_EDA.html","id":"exercise-2","dir":"Articles","previous_headings":"Exploring distributional shape > Bivariate","what":"Exercise","title":"S2_EDA: Exploratory data analysis and visualization","text":"8. (advanced): Produce shiny app controller bandwidth density estimator viewing bivariate structure faithful data. ```","code":""},{"path":"/articles/S2_EDA.html","id":"visualizing-data-in-the-context-of-the-genome","dir":"Articles","previous_headings":"","what":"Visualizing data in the context of the genome","title":"S2_EDA: Exploratory data analysis and visualization","text":"use https://vjcitn.shinyapps.io/tnt4dn8 look GWAS eQTL data jointly.","code":""},{"path":"/articles/S3_distclust.html","id":"road-map","dir":"Articles","previous_headings":"","what":"Road map","title":"S3: Distances, clustering","text":"Distances high-dimensional spaces Criteria agglomerative clustering interactive heatmap comparing Euclidean correlation (1-cor) distances comparing agglomeration methods silhouette measure clustering adequacy","code":""},{"path":"/articles/S3_distclust.html","id":"cluster-analysis-concepts","dir":"Articles","previous_headings":"","what":"Cluster analysis concepts","title":"S3: Distances, clustering","text":"need identify groups similar observations arises many contexts – ultimately service clarifying sources variability, building power statistical comparisons. Hierarchical clustering N multivariate observations conducted starting N clusters proceeding M clusters M-1 (M=N, …, 2) clusters merging two members separated least pairwise distances.","code":""},{"path":[]},{"path":[]},{"path":"/articles/S3_distclust.html","id":"application-inferring-steps-in-tumor-metastasis-in-a-breast-cancer-patient","dir":"Articles","previous_headings":"","what":"Application: inferring steps in tumor metastasis in a breast cancer patient","title":"S3: Distances, clustering","text":"’ll examine data distributed 2021 Genome Biology paper Gabor Marth lab. Clinical sequence interventions. Event sequence.","code":""},{"path":"/articles/S3_distclust.html","id":"a-view-of-copy-number-aberrations-for-1mb-tiling","dir":"Articles","previous_headings":"Application: inferring steps in tumor metastasis in a breast cancer patient","what":"A view of copy number aberrations for 1Mb tiling","title":"S3: Distances, clustering","text":"28 tumors sampled sequenced rapid autopsy procedure. Copy number variation assessed using FACETS. tissues tumors taken Br (Breast), Bo (Bone), Bn (Brain), Ln (Lung), Lv (Liver), Pa (Pancreas), Ly (Lymph nodes), Kd (Kidney) plotly-based visualization (vertical) ordering tissues chosen exemplify certain similarities. example block blue chr10 seen three samples. indication deletion.","code":""},{"path":"/articles/S3_distclust.html","id":"a-cluster-analysis-proposed-in-support-of-the-evolutionary-map","dir":"Articles","previous_headings":"Application: inferring steps in tumor metastasis in a breast cancer patient","what":"A cluster analysis proposed in support of the evolutionary map","title":"S3: Distances, clustering","text":"code lightly modified script distributed https://github.com/xiaomengh/tumor-evo-rapid-autopsy.git.","code":"suppressPackageStartupMessages({  library(csamaDist)  library(bioDist)  library(cluster) }) data(cnv_log_R) data = cnv_log_R samples = c('Ln7','Ln9','Ln1','BrM','BrP',            'Ln11','Ly2','Ln3',            'Bo3','Ln10','Bo1','Ln8','Lv3','Ln5','Bo2','Bn2','Bn1','Bn3','Bn4','Ln2',            'Ly1','Ln6',            'Kd1','Ln4','Lv4','Lv2','Lv1','Pa1') rownames(data) = samples d = dist(data, method=\"euclidean\") fit = hclust(d, method=\"average\") # the following line changes the order of the samples to produce the Fig.S3B but doesn't change the phylogenetic relationship fit$order = c(1,4,2,5,3,13,10,20,16,11,12,15,9,17,19,14,18,21,22,7,6,8,25,27,26,28,23,24) plot(fit)"},{"path":[]},{"path":"/articles/S3_distclust.html","id":"comparing-euclidean-and-correlation-distances","dir":"Articles","previous_headings":"Application: inferring steps in tumor metastasis in a breast cancer patient > Drilling down on the clustering","what":"Comparing Euclidean and Correlation distances","title":"S3: Distances, clustering","text":"given correlation distance value, can wide variation euclidean distance, vice versa. Open question: distance metric relevant biological interpretation CNV?","code":"cd = cor.dist(cnv_log_R) # from bioDist ed = dist(cnv_log_R) plot(as.numeric(ed), as.numeric(cd), xlab=\"All pairwise Euclidean distances\", ylab=\"All pairwise correlation distances\")"},{"path":"/articles/S3_distclust.html","id":"a-pair-with-discrepant-correlation-and-euclidean-distance-values-over-entire-genome","dir":"Articles","previous_headings":"Application: inferring steps in tumor metastasis in a breast cancer patient > Drilling down on the clustering","what":"A pair with discrepant correlation and euclidean distance values (over entire genome)","title":"S3: Distances, clustering","text":"’ll look first 100Mb chr1.","code":"plot(cnv_log_R[\"Ly1\",1:100],pch=19, main=\"chr1, first 100Mb\", ylab=\"FACETS CNV log R\", xlab=\"chr1\") points(cnv_log_R[\"Ln1\",1:100], col=\"red\",pch=19) legend(60, -.5, pch=19, col=c(\"black\", \"red\"), legend=c(\"Ly1\", \"Ln1\")) #cor(cnv_log_R[\"Ly1\", 1:100], cnv_log_R[\"Ln1\", 1:100]) edist = function(x,y) sqrt(sum((x-y)^2)) edist(cnv_log_R[\"Ly1\", 1:100], cnv_log_R[\"Ln1\", 1:100]) ## [1] 1.685992 plot(jitter(cnv_log_R[\"Ly1\", 1:100]), cnv_log_R[\"Ln1\", 1:100], xlab=\"Ly1\", ylab=\"Ln1\") abline(0,1)"},{"path":"/articles/S3_distclust.html","id":"redo-clustering-with-alternative-distance-and-agglomeration-method","dir":"Articles","previous_headings":"Application: inferring steps in tumor metastasis in a breast cancer patient > Drilling down on the clustering","what":"Redo clustering with alternative distance and agglomeration method","title":"S3: Distances, clustering","text":"","code":"fit2 = hclust(cd, method=\"ward.D2\") plot(fit2) abline(h=.3, lty=2)"},{"path":"/articles/S3_distclust.html","id":"silhouette-measure","dir":"Articles","previous_headings":"Application: inferring steps in tumor metastasis in a breast cancer patient > Drilling down on the clustering","what":"Silhouette measure","title":"S3: Distances, clustering","text":"?silhouette cluster library:","code":"For each observation i, the _silhouette width_ s(i) is defined as follows:       Put a(i) = average dissimilarity between i and all other points of      the cluster to which i belongs (if i is the _only_ observation in      its cluster, s(i) := 0 without further calculations).  For all      _other_ clusters C, put d(i,C) = average dissimilarity of i to all      observations of C.  The smallest of these d(i,C) is b(i) := \\min_C      d(i,C), and can be seen as the dissimilarity between i and its      \"neighbor\" cluster, i.e., the nearest one to which it does _not_      belong.  Finally,                     s(i) := ( b(i) - a(i) ) / max( a(i), b(i) ).                     'silhouette.default()' is now based on C code donated by Romain      Francois (the R version being still available as      'cluster:::silhouette.default.R').       Observations with a large s(i) (almost 1) are very well clustered,      a small s(i) (around 0) means that the observation lies between      two clusters, and observations with a negative s(i) are probably      placed in the wrong cluster. ct1 = cutree(fit2, h=.3) c2 = cnv_log_R rownames(c2) = paste(rownames(c2), as.numeric(ct1)) sil = silhouette(ct1, cd) plot(sil)"},{"path":"/articles/S3_distclust.html","id":"exercises","dir":"Articles","previous_headings":"Application: inferring steps in tumor metastasis in a breast cancer patient > Drilling down on the clustering","what":"Exercises","title":"S3: Distances, clustering","text":"1: install bioDist vjcitn/csamaDist. Use code: Comment qualitative differences clusterings. 2: produce report silhouette measurement second clustering. Produce three-cluster partition hc1 obtain silhouette display.","code":"library(csamaDist) data(cnv_log_R) hc1 = hclust(dist(cnv_log_R[1:8,])) hc2 = hclust(bioDist::cor.dist(cnv_log_R[1:8,]), method=\"ward.D2\")  opar = par(no.readonly=TRUE) par(mfrow=c(1,2), mar=c(4,3,1,1)) plot(hc1, main=\"Euc, complete\") plot(hc2, main=\"1-Cor, Ward's D2\") par(opar) assn =  cutree(hc2, h=.25) plot(silhouette( assn, bioDist::cor.dist(cnv_log_R[1:8,])))"},{"path":[]},{"path":"/articles/S3_hypothesis_testing.html","id":"the-fairness-hypothesis","dir":"Articles","previous_headings":"Inference","what":"The fairness hypothesis","title":"S3_inference: Definitions and exercises","text":"Let \\(C_1\\) denote suit top card revealed fair shuffle. can state hypothesis fairness deck repeated draws top card shuffling \\[ H_0: Pr(C_1 = \\heartsuit) = Pr(C_1 = \\diamondsuit) = Pr(C_1 = \\clubsuit) = Pr(C_1 = \\spadesuit) = 1/4 \\] frequentist framework statistical inference, define procedures testing (null) hypotheses specified error probabilities. Type error occurs null hypothesis true test results assertion false. Traditionally try keep probability Type errors 5%. Type II error occurs null hypothesis false test result assertion false. Traditionally try keep probability Type II errors 20%.","code":""},{"path":"/articles/S3_hypothesis_testing.html","id":"exercises","dir":"Articles","previous_headings":"Inference > The fairness hypothesis","what":"Exercises","title":"S3_inference: Definitions and exercises","text":"14: Propose test \\(H_0\\) given . Assume results top card draws 100 shuffles. 15: Consider approach testing \\(H_0\\) basis top card draws \\(N\\) shuffles:","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"/articles/S4_hypothesis_testing.html","id":"basic-concepts","dir":"Articles","previous_headings":"Inference","what":"Basic concepts","title":"S4_inference: Definitions and exercises","text":"going focus framework known “null hypothesis significance testing”. “null hypothesis” typically denoted \\(H_0\\) expresses claim often involves lack effect experimental intervention outcome interest. “alternative hypothesis” denoted \\(H_1\\), express specific measurable impact intervention. case card deck chapter S1, typical postulate \\(H_0\\): deck fair, one card combination face suit \\(H_1\\): deck tampered excess hearts Note case, \\(H_1\\) fairly vague – less specific, referring lack fairness. \\(H_1\\) specifies suit excess representation deck, specific severe tampering . experimental design, important clear view sources variation outcome interest, magnitude effect intervention scientifically important. focus can establish operating characteristics proposed experiment. significance testing framework, formalize operating characteristics probabilities inferential error, described . plunge formalities, let’s recall work deck cards. create fair deck: Now let’s create severely biased deck: tools shuffling drawing top card: Let’s make reproducible random draw: Exercise: create vector holding 100 draws biased deck. Estimate probability suit top card diamond. Use test suit draw. example: Note: starting set.seed(2345), estimated probability diamond suit top draw found 0.16.","code":"library(CSHstats) d = build_deck() table(suits(d), faces(d)) ##     ##     10 2 3 4 5 6 7 8 9 A J K Q ##   ♡  1 1 1 1 1 1 1 1 1 1 1 1 1 ##   ♢  1 1 1 1 1 1 1 1 1 1 1 1 1 ##   ♣  1 1 1 1 1 1 1 1 1 1 1 1 1 ##   ♤  1 1 1 1 1 1 1 1 1 1 1 1 1 bd = d  # just a copy bd[18] = bd[3] bd[19] = bd[3] bd[20] = bd[4] bd[21] = bd[5] table(suits(bd), faces(bd)) ##     ##     10 2 3 4 5 6 7 8 9 A J K Q ##   ♡  1 1 1 3 2 2 1 1 1 1 1 1 1 ##   ♢  1 1 1 1 1 0 0 0 0 1 1 1 1 ##   ♣  1 1 1 1 1 1 1 1 1 1 1 1 1 ##   ♤  1 1 1 1 1 1 1 1 1 1 1 1 1 shuffle_deck = function(d) sample(d, size=length(d), replace=FALSE) top_draw = function(d) shuffle_deck(d)[1] set.seed(1234) t1 = top_draw(bd) t1 ## [1] \"3 ♣\" diamond_sign = function() \"\\U2662\" t1 == diamond_sign() ## [1] FALSE"},{"path":"/articles/S4_hypothesis_testing.html","id":"the-fairness-hypothesis","dir":"Articles","previous_headings":"Inference","what":"The fairness hypothesis","title":"S4_inference: Definitions and exercises","text":"Let \\(C_1\\) denote suit top card revealed fair shuffle. can state hypothesis fairness deck repeated draws top card shuffling \\[ H_0: Pr(C_1 = \\heartsuit) = Pr(C_1 = \\diamondsuit) = Pr(C_1 = \\clubsuit) = Pr(C_1 = \\spadesuit) = 1/4 \\]","code":""},{"path":"/articles/S4_hypothesis_testing.html","id":"operating-characteristics-type-i-and-type-ii-errors","dir":"Articles","previous_headings":"Inference","what":"Operating characteristics: Type I and Type II errors","title":"S4_inference: Definitions and exercises","text":"frequentist framework statistical inference, define procedures testing (null) hypotheses specified error probabilities. Type error occurs null hypothesis true test results assertion false. Traditionally try keep probability Type errors 5%. Type II error occurs null hypothesis false test result assertion false. Traditionally try keep probability Type II errors 20%. Synonyms: often use size synonymous Type error probability, power one minus Type II error probability","code":""},{"path":"/articles/S4_hypothesis_testing.html","id":"exercise","dir":"Articles","previous_headings":"Inference > Operating characteristics: Type I and Type II errors","what":"Exercise","title":"S4_inference: Definitions and exercises","text":"1: Propose procedure testing \\(H_0: Pr(C_1 = \\heartsuit) = 1/4\\). Assume results top card draws N shuffles. procedure look like?","code":""},{"path":"/articles/S4_hypothesis_testing.html","id":"simulating-a-series-of-experiments-parameter-estimate","dir":"Articles","previous_headings":"Inference","what":"Simulating a series of experiments; parameter estimate","title":"S4_inference: Definitions and exercises","text":"100 shuffles estimate probability heart. experimental result consistent \\(H_0\\)? larger sample size:","code":"suppressPackageStartupMessages({ suppressMessages({ library(CSHstats) library(EnvStats) }) }) d = build_deck() shuffle_deck = function(d) sample(d, size=length(d), replace=FALSE) heart_sign = function() \"\\U2661\" set.seed(4141) top_draw = function(d) shuffle_deck(d)[1] N = 100 mydat = replicate(N, suits(top_draw(d))==heart_sign()) phat = function(dat) sum(dat)/length(dat) phat(mydat) ## [1] 0.35 N = 1000 mydat = replicate(N, suits(top_draw(d))==heart_sign()) sum(mydat)/length(mydat) ## [1] 0.243"},{"path":"/articles/S4_hypothesis_testing.html","id":"an-improvised-test","dir":"Articles","previous_headings":"Inference","what":"An improvised test","title":"S4_inference: Definitions and exercises","text":"Let’s use procedure |phat(dat)-.25|>.01 criterion rejecting \\(H_0: Pr(C_1 = \\heartsuit) = 1/4\\) Type error rate fair deck experiment based 100 shuffles? Parameterize “delta”, 0.01 previous run. Let’s get rejection rate estimates two sample sizes: home-cooked test Type error rate much high standard practice, depends sample size.","code":"N = 100 NSIM = 1000 tsts = replicate(NSIM,    abs(mean(replicate(N, suits(top_draw(d))==heart_sign()))-.25)>.01) mean(tsts) ## [1] 0.911 prej = function(delta=.01, nullval=.25, NSIM, N) {  mean(replicate(NSIM,    abs(mean(replicate(N, suits(top_draw(d))==heart_sign()))-nullval)>delta)) } prej(delta=.02, NSIM=1000, N=100) ## [1] 0.655 prej(delta=.02, NSIM=1000, N=500) ## [1] 0.323"},{"path":"/articles/S4_hypothesis_testing.html","id":"a-properly-calibrated-procedure","dir":"Articles","previous_headings":"Inference","what":"A properly calibrated procedure","title":"S4_inference: Definitions and exercises","text":"Let’s instead use built procedure testing hypotheses binomial outcomes. brej function defined two layers: - inner: given N draws, test sum number hearts consistent specified null value (1/4 default), specified Type error rate (argument alpha) - outer: replicate inner process NSIM times obtain NSIM indicator variables taking value 1 test rejected 0 otherwise estimate rejection probabilities taking means indicator variables different experimental replication setups. indication procedure stabilizes Type error rate different designs (sample sizes) can accommodate different significance levels.","code":"brej = function(deck, nullval=.25, alpha=0.05, NSIM, N) {  replicate(NSIM, {    dat = replicate(N, suits(top_draw(deck))==heart_sign())    binom.test(sum(dat), N, nullval)$p.value < alpha    }) } mean(brej(d,NSIM=1000, N=100)) ## [1] 0.054 mean(brej(d,NSIM=1000, N=500)) ## [1] 0.06 mean(brej(d,NSIM=1000, N=100, alpha=0.01)) ## [1] 0.006"},{"path":"/articles/S4_hypothesis_testing.html","id":"power-curve","dir":"Articles","previous_headings":"","what":"Power curve","title":"S4_inference: Definitions and exercises","text":"often useful sketch power test procedure series values element experimental/testing setup. ’ll consider power reject null varies sample size, two alternatives null. make biased deck switching one diamond heart. deck, probability heart 14/52, greater 1/4. kind sample size need get good power detect exception?  two switches, probability heart increases 15/52.  Finally, show null hypothesis (deck fair) rejection rate approximately 0.05 range sample sizes.","code":"bd = d bd[18] = bd[4] table(suits(bd), faces(bd)) ##     ##     10 2 3 4 5 6 7 8 9 A J K Q ##   ♡  1 1 1 1 2 1 1 1 1 1 1 1 1 ##   ♢  1 1 1 1 1 0 1 1 1 1 1 1 1 ##   ♣  1 1 1 1 1 1 1 1 1 1 1 1 1 ##   ♤  1 1 1 1 1 1 1 1 1 1 1 1 1 ssizes = c(10,50, 100, 300,750) rejps = sapply(ssizes, function(x) mean(brej(bd, NSIM=100, N=x))) plot(ssizes, rejps, main=\"detect one extra heart\") bd[19] = bd[5] table(suits(bd), faces(bd)) ##     ##     10 2 3 4 5 6 7 8 9 A J K Q ##   ♡  1 1 1 1 2 2 1 1 1 1 1 1 1 ##   ♢  1 1 1 1 1 0 0 1 1 1 1 1 1 ##   ♣  1 1 1 1 1 1 1 1 1 1 1 1 1 ##   ♤  1 1 1 1 1 1 1 1 1 1 1 1 1 rejps = sapply(ssizes, function(x) mean(brej(bd, NSIM=100, N=x))) plot(ssizes, rejps, main=\"detect two extra hearts\") rejps_n = sapply(ssizes, function(x) mean(brej(d, NSIM=100, N=x))) plot(ssizes, rejps_n, main=\"preserve Type I error rate\", ylim=c(0,1)) abline(h=0.05, lty=2)"},{"path":"/articles/S4_hypothesis_testing.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"S4_inference: Definitions and exercises","text":"Classical frequentist inference aims control probabilities errors Type (reject null hypothesis true) Type II (fail reject false null). power test procedure one minus Type II error probability. case card deck, hypothesis fairness deck shuffling procedure entails probability drawing heart shuffle 1/4 experimental setup entails probability distribution sum number hearts drawn top card N shuffling events binomial(N,1/4) sum number hearts observed, number trials, parameter value defining null hypothesis, desired type error rate, produces critical region using binomial distribution sum falls critical region, null hypothesis rejected Verification achievement desired Type Type II error probabilities testing procedure can pursued via simulation power curve sequence experimental conditions can produced via simulation Analytical tools power computation also available. example, EnvStats package, find exercise reconcile results simulation-based results given . Another find sample size needed achieve 80% power detect single switch diamond heart otherwise fair deck.","code":"library(EnvStats) propTestPower(750, p.or.p1=13/52, p0=.25 ) ## [1] 0.05 propTestPower(750, p.or.p1=14/52, p0=.25 ) ## [1] 0.2348789 propTestPower(750, p.or.p1=15/52, p0=.25 ) ## [1] 0.6742566"},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"/articles/S5_linear_models.html","id":"error-rates-size-and-power","dir":"Articles","previous_headings":"Review of some inference concepts","what":"Error rates: size and power","title":"S5_inference: FDR, linear models, and GLMs","text":"average value quantity change treatment? changing composition deck cards affect probability seeing given suit drawing card? Type error: reject null actually true Type II error: fail reject null actually false","code":""},{"path":"/articles/S5_linear_models.html","id":"p-value-a-condensation-of-the-data-and-assumptions","dir":"Articles","previous_headings":"Review of some inference concepts","what":"p-value: a condensation of the data and assumptions","title":"S5_inference: FDR, linear models, and GLMs","text":"p-value test probability observing statistic seen experiment extreme value statistic given null hypothesis true. two-sided p-value obtained via results 27 hearts seen, 22 fewer hearts seen “extreme” observed. p-value probability, value reflects uncertainty assessment relation data null hypothesis. Large p-values suggest much reason use data reject null hypothesis; small p-values suggest either null hypothesis false rare event occurred.","code":"binom.test(27, 100, 1/4) ##  ##  Exact binomial test ##  ## data:  27 and 100 ## number of successes = 27, number of trials = 100, p-value = 0.6446 ## alternative hypothesis: true probability of success is not equal to 0.25 ## 95 percent confidence interval: ##  0.1860664 0.3680163 ## sample estimates: ## probability of success  ##                   0.27 sum(dbinom(27:100, 100, .25))+sum(dbinom(1:22,100, .25)) ## [1] 0.6446304"},{"path":"/articles/S5_linear_models.html","id":"confidence-interval-an-alternative-expression-of-uncertainty","dir":"Articles","previous_headings":"Review of some inference concepts","what":"Confidence interval: an alternative expression of uncertainty","title":"S5_inference: FDR, linear models, and GLMs","text":"observed 30 hearts 100 top-card draws, test report : confidence interval random interval derived data property include true value population parameter interest specified probability.  Exercise: value statistic reject null hypothesis probability 1/4 heart? Exercise: Use 99% coverage probability produce display.","code":"binom.test(30, 100, 1/4) ##  ##  Exact binomial test ##  ## data:  30 and 100 ## number of successes = 30, number of trials = 100, p-value = 0.2491 ## alternative hypothesis: true probability of success is not equal to 0.25 ## 95 percent confidence interval: ##  0.2124064 0.3998147 ## sample estimates: ## probability of success  ##                    0.3 stats = c(27:42) tests = lapply(stats, function(x) binom.test(x, 100, .25)) low = sapply(tests, function(x) x$conf.int[1]) hi = sapply(tests, function(x) x$conf.int[2]) ests = sapply(tests, \"[[\", \"statistic\") plot(stats, ests, ylim=c(0, .6), main=\"95% Confidence intervals\", xlab=\"# hearts seen in 100 draws\",   ylab=\"estimated proportion of hearts\") segments( stats, low, stats, hi) abline(h=.25, lty=2)"},{"path":"/articles/S5_linear_models.html","id":"multiple-comparisons","dir":"Articles","previous_headings":"","what":"Multiple comparisons","title":"S5_inference: FDR, linear models, and GLMs","text":"Everything ’ve seen thus far looks single test various ways. hallmark work genomics necessity performing many tests, large number features hypotheses play. Fact: many tests conducted true null hypotheses, distribution collection p-values thereby obtained uniform (0,1]. illustrate produce 10000 samples N(100,1) conduct t test null mean 100, yielding 10000 p-values.  display shows limitation density estimation fixed interval – “boundary effect”, data “” boundary get sparser get closer boundary. consistent property uniform distribution [0,1]: true density 1.","code":"many_x = replicate(10000, rnorm(10, 100)) many_p = apply(many_x, 2, function(x) t.test(x, mu=100)$p.value) plot(density(many_p, from=0, to=1))"},{"path":"/articles/S5_linear_models.html","id":"bonferronis-correction-to-achieve-family-wise-error-rate-fwer-control","dir":"Articles","previous_headings":"Multiple comparisons","what":"Bonferroni’s correction to achieve Family-Wise Error Rate (FWER) control","title":"S5_inference: FDR, linear models, and GLMs","text":"concept Type error general, deployed define operating characteristics kind inference procedure. table frequently seen discussions multiple testing, Holmes Huber’s Modern Statistics Modern Biology. decisiontab Suppose \\(m\\) hypotheses test wish overall probability family tests Type error rate \\(\\alpha\\). Using table symbols, \\(Pr(V>0) < \\alpha\\). Bonferroni’s method declare significance tests \\(p\\)-value less \\(\\alpha/m\\). procedure (related improvements FWER control) also implies transformation \\(p\\)-values, see shortly.","code":""},{"path":"/articles/S5_linear_models.html","id":"false-discovery-rate","dir":"Articles","previous_headings":"Multiple comparisons","what":"False discovery rate","title":"S5_inference: FDR, linear models, and GLMs","text":"Referring table , false discovery rate (FDR) expected value \\(V/max(R,1)\\). \\(R = 0\\), \\(V = 0\\) FDR zero. Otherwise, can think FDR kind budget: reject \\(R\\) hypotheses expection \\(V\\) false. “following ” confirmatory experiments \\(R\\) genes can afford “wasting” \\(V\\) tests, accept FDR \\(V/R\\). course probabilistic characteristics procedures hold stated probabilities assumptions satisfied.","code":""},{"path":"/articles/S5_linear_models.html","id":"adjusted-p-values","dir":"Articles","previous_headings":"Multiple comparisons","what":"Adjusted p-values","title":"S5_inference: FDR, linear models, and GLMs","text":"Since accustomed single-number summary \\(p\\), typical base rejection decisions transformed \\(p\\)-values. multtest package takes care . default, mt.rawp2adp produces number adjustments – concerned present columns rawp (p-values sorted lowest ), Bonferroni (gives transformation \\(p\\) min(1, mp)$ \\(m\\) tests), BH (Benjamini-Hochberg FDR transformation). implication table FWER control 0.05, data support rejection hypotheses (assert mean sample 10 random normals equal 100). Likewise FDR control value 0.77. However, see accepted FDR control 0.8, reject 8 hypotheses. Exercise: replace first p-value many_p 1e-6 obtain associated adjusted p-values.","code":"library(multtest) options(digits=3) tab = mt.rawp2adjp(many_p) head(tab[[1]],10) ##           rawp Bonferroni  Holm Hochberg SidakSS SidakSD    BH BY   ABH ##  [1,] 6.96e-05      0.696 0.696    0.696   0.501   0.501 0.696  1 0.696 ##  [2,] 2.14e-04      1.000 1.000    1.000   0.883   0.883 0.745  1 0.745 ##  [3,] 2.49e-04      1.000 1.000    1.000   0.917   0.917 0.745  1 0.745 ##  [4,] 4.20e-04      1.000 1.000    1.000   0.985   0.985 0.745  1 0.745 ##  [5,] 4.44e-04      1.000 1.000    1.000   0.988   0.988 0.745  1 0.745 ##  [6,] 6.58e-04      1.000 1.000    1.000   0.999   0.999 0.745  1 0.745 ##  [7,] 7.99e-04      1.000 1.000    1.000   1.000   1.000 0.745  1 0.745 ##  [8,] 8.07e-04      1.000 1.000    1.000   1.000   1.000 0.745  1 0.745 ##  [9,] 8.82e-04      1.000 1.000    1.000   1.000   1.000 0.745  1 0.745 ## [10,] 1.04e-03      1.000 1.000    1.000   1.000   1.000 0.745  1 0.745 ##       TSBH_0.05 ##  [1,]     0.696 ##  [2,]     0.745 ##  [3,]     0.745 ##  [4,]     0.745 ##  [5,]     0.745 ##  [6,]     0.745 ##  [7,]     0.745 ##  [8,]     0.745 ##  [9,]     0.745 ## [10,]     0.745"},{"path":"/articles/S5_linear_models.html","id":"adjustments-via-weighted-fdr","dir":"Articles","previous_headings":"Multiple comparisons","what":"Adjustments via weighted FDR","title":"S5_inference: FDR, linear models, and GLMs","text":"useful review available. section addresses one methods review, found good performance, though contexts authors’ alternatives appear superior.","code":""},{"path":"/articles/S5_linear_models.html","id":"setup","dir":"Articles","previous_headings":"Multiple comparisons > Adjustments via weighted FDR","what":"Setup","title":"S5_inference: FDR, linear models, and GLMs","text":"need IHW package visualization well. ’s key resource p-values want interpret. variant-expression association tests conducted GTEx project. Thus small excerpt collection “GWAS”, outcome tissue-specific expression gene, predictors formed SNP genotypes adjustments carried GTEx analysis pipeline. ’s composite manhattan plot:","code":"library(CSHstats)  library(ggplot2)  library(plotly)  library(IHW)  library(multtest) data(gtex_exc_chr20_b38) gtdat = gtex_exc_chr20_b38 head(gtdat,3) ##   chr pos_b38       marker         rsid effectallele otherallele    eaf     p ## 1  20   61066 20:61066:T:C rs1031667049            C           T 0.0285 0.774 ## 2  20   61083 20:61083:C:T  rs956068117            T           C 0.1010 0.767 ## 3  20   61098 20:61098:C:T  rs933208784            T           C 0.0155 0.117 ##      beta     se              pheno mode       tag imputersq   n     Gene ## 1  0.0226 0.0784 ENSG00000196476.11 eqtl GTEx-eqtl        NA 515 C20orf96 ## 2  0.0123 0.0413 ENSG00000196476.11 eqtl GTEx-eqtl        NA 515 C20orf96 ## 3 -0.1615 0.1029 ENSG00000196476.11 eqtl GTEx-eqtl        NA 515 C20orf96 ##   seqnames  start    end width strand         gene_id gene_name   gene_biotype ## 1       20 270863 290778 19916      - ENSG00000196476  C20orf96 protein_coding ## 2       20 270863 290778 19916      - ENSG00000196476  C20orf96 protein_coding ## 3       20 270863 290778 19916      - ENSG00000196476  C20orf96 protein_coding ##   seq_coord_system   symbol entrezid ## 1       chromosome C20orf96   140680 ## 2       chromosome C20orf96   140680 ## 3       chromosome C20orf96   140680 pl1 = ggplot(gtdat[gtdat$p<.1,], aes(x=pos_b38, y=-log10(p), colour=Gene)) + geom_point() #ggplotly(pl1) pl1"},{"path":"/articles/S5_linear_models.html","id":"demonstration-1-use-allele-frequency-to-form-strata-for-fdr-weighting","dir":"Articles","previous_headings":"Multiple comparisons > Adjustments via weighted FDR","what":"Demonstration 1: Use allele frequency to form strata for FDR weighting","title":"S5_inference: FDR, linear models, and GLMs","text":"eaf field frequency “effect allele” reported SNP-Nexus. run IHW algorithm using variant frequency strata, conjecture rare common variants may downweighted way advantageous discovery. weighting procedure involves random resampling data, reduce risk overfitting, allow assessment consistency across multiple “folds” random subsets.  Compare weighted, unweighted, conventional rejection numbers:","code":"covariate1 = cut(gtdat$eaf, c(0,.01,.05,.1,.3,.6)) table(covariate1) ## covariate1 ##    (0,0.01] (0.01,0.05]  (0.05,0.1]   (0.1,0.3]   (0.3,0.6]  ##        3865       33697       13860       27840       20738 demo1 = ihw(gtdat$p, covariate1, alpha=0.05) basic_fdr = mt.rawp2adjp(gtdat$p) plot(demo1) newp = adj_pvalues(demo1) sum(newp < 0.05) ## [1] 276 sum(basic_fdr$adjp[,\"BH\"] < 0.05) ## [1] 257 sum(gtdat$p < 5e-8) ## [1] 68"},{"path":"/articles/S5_linear_models.html","id":"demonstration-2-use-distance-to-gene-to-weight","dir":"Articles","previous_headings":"Multiple comparisons > Adjustments via weighted FDR","what":"Demonstration 2: Use distance to gene to weight","title":"S5_inference: FDR, linear models, and GLMs","text":"","code":"dist2gene = abs(gtdat$pos_b38-gtdat$start) covariate2 = cut(dist2gene, 5) ok = which(!is.na(covariate2)) demo2 = ihw(gtdat$p[ok], covariate2[ok], alpha=0.05) plot(demo2) newp2 = adj_pvalues(demo2) sum(newp2 < 0.05) ## [1] 379"},{"path":"/articles/S5_linear_models.html","id":"optional-considerations-on-bayesian-multiple-testing","dir":"Articles","previous_headings":"Multiple comparisons","what":"Optional: Considerations on Bayesian multiple testing","title":"S5_inference: FDR, linear models, and GLMs","text":"Various comments references stackexchange address relationship inferential framework approach multiple testing. paper Gelman Loken surveys concept illuminating way. paper European Journal Epidemiology presents considerations multiple comparisons Bayesian perspective, arguing issue “surrounded confusion controversy”. simple approach visualizing 20 tests, assumed independent, frequenist framework, focusing 95% confidence intervals.  One hypothesis rejected. authors also use bayesm package produce Bayesian credible intervals assumption tests independent.  Finally, allowing high correlatedness among test statistics, Bayesian approach found avoid false rejections.  Bayesian computations involve Monte Carlo Markov Chain simulation. Background can obtained Statistical Rethinking R. McElreath.","code":"# from https://link.springer.com/content/pdf/10.1007/s10654-019-00517-2.pdf # Sjolander and vanSteenlandt 2019 Eur J Epi  library(forestplot) library(bayesm) set.seed(1)  n = 10 J = 20 beta = 0 Y = matrix(rnorm(n*J, mean=beta), nrow=n, ncol=J) est = colMeans(Y) se = apply(X=Y, MARGIN=2, FUN=sd)/sqrt(n) q = qt(p=0.975, df=n-1) ci.l = est-q*se ci.u = est+q*se  forestplot(  labeltext=rep(\"\", J),  mean=est, lower=ci.l,  upper=ci.u, ci.vertices=TRUE,  boxsize=0.3, txt_gp=fpTxtGp(cex=2),  xticks=seq(-1.5,1.5,.5), title=\"Frequentist 95% confidence intervals\") forestplot(  labeltext=rep(\"\", J),  mean=est, lower=ci.l,  upper=ci.u, ci.vertices=TRUE,  boxsize=0.3, txt_gp=fpTxtGp(cex=2),  xticks=seq(-1.5,1.5,.5), title=\"Bayesian, assume correlated tests\")"},{"path":[]},{"path":"/articles/S5_linear_models.html","id":"data-setup","dir":"Articles","previous_headings":"Linear models","what":"Data setup","title":"S5_inference: FDR, linear models, and GLMs","text":"Let’s return TCGA ACC FOS EGR1 data used study correlation.  found log transformation useful ameliorating overplotting near origin:  ACC data annotated MultiAssayExperiment, mutation summary called OncoSign available.","code":"data(fos_ex) data(egr1_ex) fedf = data.frame(FOS=fos_ex, EGR1=egr1_ex) library(ggplot2) ggplot(fedf, aes(x=EGR1, y=FOS)) + geom_point() ggplot(fedf, aes(x=EGR1, y=FOS)) + geom_point() + scale_x_log10() + scale_y_log10() library(MultiAssayExperiment) library(CSHstats) data(accex) accex ## A MultiAssayExperiment object of 1 listed ##  experiment with a user-defined name and respective class. ##  Containing an ExperimentList class object of length 1: ##  [1] ACC_RNASeq2GeneNorm-20160128: SummarizedExperiment with 20501 rows and 79 columns ## Functionality: ##  experiments() - obtain the ExperimentList instance ##  colData() - the primary/phenotype DataFrame ##  sampleMap() - the sample coordination DataFrame ##  `$`, `[`, `[[` - extract colData columns, subset, or experiment ##  *Format() - convert into a long or wide DataFrame ##  assays() - convert ExperimentList to a SimpleList of matrices ##  exportClass() - save data to flat files table(accex$OncoSign, exclude=NULL) ##  ##          CN1          CN2       CTNNB1   TERT/ZNRF3     TP53/NF1 Unclassified  ##            4           13            5           17           21           14  ##         <NA>  ##            5"},{"path":"/articles/S5_linear_models.html","id":"re-expressing-the-two-sample-test","dir":"Articles","previous_headings":"Linear models","what":"Re-expressing the two-sample test","title":"S5_inference: FDR, linear models, and GLMs","text":"focus two OncoSign classes, asking whether average log EGR1 expression different .","code":"hasonc = which(!is.na(accex$OncoSign)) ndf = data.frame(logEGR1 = log(egr1_ex[hasonc]), oncosign=accex$OncoSign[hasonc]) tpvtert = ndf |> dplyr::filter(oncosign %in% c(\"TP53/NF1\", \"TERT/ZNRF3\")) ggplot(tpvtert, aes(y=logEGR1, x=factor(oncosign))) + geom_boxplot()"},{"path":"/articles/S5_linear_models.html","id":"there-are-two-t-tests","dir":"Articles","previous_headings":"Linear models > Re-expressing the two-sample test","what":"There are two t tests","title":"S5_inference: FDR, linear models, and GLMs","text":"","code":"options(digits=6) t.test(logEGR1~oncosign, data=tpvtert, var.equal=TRUE) ##  ##  Two Sample t-test ##  ## data:  logEGR1 by oncosign ## t = 1.038, df = 36, p-value = 0.306 ## alternative hypothesis: true difference in means between group TERT/ZNRF3 and group TP53/NF1 is not equal to 0 ## 95 percent confidence interval: ##  -0.374199  1.158895 ## sample estimates: ## mean in group TERT/ZNRF3   mean in group TP53/NF1  ##                  7.35317                  6.96083 t.test(logEGR1~oncosign, data=tpvtert, var.equal=FALSE) # more general ##  ##  Welch Two Sample t-test ##  ## data:  logEGR1 by oncosign ## t = 1.046, df = 35.19, p-value = 0.303 ## alternative hypothesis: true difference in means between group TERT/ZNRF3 and group TP53/NF1 is not equal to 0 ## 95 percent confidence interval: ##  -0.369343  1.154039 ## sample estimates: ## mean in group TERT/ZNRF3   mean in group TP53/NF1  ##                  7.35317                  6.96083"},{"path":"/articles/S5_linear_models.html","id":"the-linear-model-has-one-of-these-as-a-special-case","dir":"Articles","previous_headings":"Linear models > Re-expressing the two-sample test","what":"The linear model has one of these as a special case","title":"S5_inference: FDR, linear models, and GLMs","text":"general form linear model statistics “dependent variable” \\(y\\) \\[ y = \\alpha + x' \\beta + e \\] \\(x\\) p-vector “covariates” – measurements believed affect mean value \\(y\\), also known “independent variables”, \\(e\\) zero-mean disturbance residual fixed dispersion. parameters interest \\(\\alpha\\) (often called intercept) \\(\\beta\\). Parameter interpretation terms units response relative differences samples different values \\(x\\) always explicit.","code":""},{"path":"/articles/S5_linear_models.html","id":"a-dummy-variable-design-matrix","dir":"Articles","previous_headings":"Linear models > Re-expressing the two-sample test > The linear model has one of these as a special case","what":"A dummy variable design matrix","title":"S5_inference: FDR, linear models, and GLMs","text":"","code":"mm = model.matrix(~oncosign, data=tpvtert) head(mm) ##   (Intercept) oncosignTP53/NF1 ## 1           1                1 ## 2           1                1 ## 3           1                0 ## 4           1                1 ## 5           1                0 ## 6           1                1 tail(mm) ##    (Intercept) oncosignTP53/NF1 ## 33           1                0 ## 34           1                1 ## 35           1                0 ## 36           1                0 ## 37           1                1 ## 38           1                1"},{"path":"/articles/S5_linear_models.html","id":"recovering-the-t-test-with-lm","dir":"Articles","previous_headings":"Linear models > Re-expressing the two-sample test > The linear model has one of these as a special case","what":"Recovering the t test with lm","title":"S5_inference: FDR, linear models, and GLMs","text":"Exercise: t test recovered analysis? Exercise: Conduct associated Wilcoxon test. Exercise: Conduct Gaussian rank test mentioned Gelman blog “Don’t Wilcoxon”. Extra goody: produce coefficients “hand”: programs \\((X'X)^{-1}X'y\\)","code":"tpvtert |> lm(logEGR1~oncosign, data=_) |> summary() ##  ## Call: ## lm(formula = logEGR1 ~ oncosign, data = tpvtert) ##  ## Residuals: ##    Min     1Q Median     3Q    Max  ## -2.287 -0.721  0.167  0.625  2.549  ##  ## Coefficients: ##                  Estimate Std. Error t value Pr(>|t|)     ## (Intercept)         7.353      0.281   26.17   <2e-16 *** ## oncosignTP53/NF1   -0.392      0.378   -1.04     0.31     ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ##  ## Residual standard error: 1.16 on 36 degrees of freedom ## Multiple R-squared:  0.0291, Adjusted R-squared:  0.00209  ## F-statistic: 1.08 on 1 and 36 DF,  p-value: 0.306 solve(t(mm)%*%mm)%*%t(mm)%*%tpvtert$logEGR1 ##                       [,1] ## (Intercept)       7.353174 ## oncosignTP53/NF1 -0.392348"},{"path":"/articles/S5_linear_models.html","id":"analysis-of-variance-f-test","dir":"Articles","previous_headings":"Linear models","what":"Analysis of variance: F test","title":"S5_inference: FDR, linear models, and GLMs","text":"analysis variance focused comparison multiple groups. EGR1-OncoSign relationship reasonable example used.","code":"ggplot(ndf, aes(y=logEGR1, x=factor(oncosign))) + geom_boxplot() nda = aov(logEGR1~factor(oncosign), data=ndf) nda ## Call: ##    aov(formula = logEGR1 ~ factor(oncosign), data = ndf) ##  ## Terms: ##                 factor(oncosign) Residuals ## Sum of Squares           11.3084   92.1642 ## Deg. of Freedom                5        68 ##  ## Residual standard error: 1.1642 ## Estimated effects may be unbalanced summary(nda) ##                  Df Sum Sq Mean Sq F value Pr(>F) ## factor(oncosign)  5   11.3    2.26    1.67   0.15 ## Residuals        68   92.2    1.35"},{"path":"/articles/S5_linear_models.html","id":"linear-regression-parameter-estimation-goodness-of-fit","dir":"Articles","previous_headings":"Linear models","what":"Linear regression, parameter estimation, goodness of fit","title":"S5_inference: FDR, linear models, and GLMs","text":"common applications linear modeling take advantage continuity predictor. simple linear regression (one continuous covariate), interpretation \\(\\beta\\) : difference mean \\(y\\) associated unit difference \\(x\\). Regarding log EGR1 predictor log FOS, ","code":"m1 = lm(log(fos_ex)~log(egr1_ex)) summary(m1) ##  ## Call: ## lm(formula = log(fos_ex) ~ log(egr1_ex)) ##  ## Residuals: ##    Min     1Q Median     3Q    Max  ## -1.545 -0.482  0.198  0.476  1.397  ##  ## Coefficients: ##              Estimate Std. Error t value Pr(>|t|)     ## (Intercept)    2.2980     0.4975    4.62  1.5e-05 *** ## log(egr1_ex)   0.6895     0.0692    9.96  1.8e-15 *** ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ##  ## Residual standard error: 0.727 on 77 degrees of freedom ## Multiple R-squared:  0.563,  Adjusted R-squared:  0.557  ## F-statistic: 99.1 on 1 and 77 DF,  p-value: 1.75e-15 plot(m1)"},{"path":[]},{"path":"/articles/S5_linear_models.html","id":"glms-binary-counted-non-gaussian-responses","dir":"Articles","previous_headings":"","what":"GLMs: binary, counted, non-Gaussian responses","title":"S5_inference: FDR, linear models, and GLMs","text":"family generalized linear models (GLM) embraces number approaches statistical inference associations non-Gaussian responses general configurations covariates.","code":""},{"path":"/articles/S5_linear_models.html","id":"prologue-fishers-exact-test-for-2-x-2-table","dir":"Articles","previous_headings":"GLMs: binary, counted, non-Gaussian responses","what":"Prologue: Fisher’s exact test for 2 x 2 table","title":"S5_inference: FDR, linear models, and GLMs","text":"’ll work FOS EGR1 artificial way. ’ll suppose criteria “activation”: FOS activated RNA-seq measurement ACC exceeds 1000 EGR1 activated RNA-seq measurement ACC exceeds 2000 form 2 x 2 cross classification tumors using definitions: basic question whether two events “EGR1 activated” “FOS activated” statistically associated ACC tumors. Fisher’s exact test evaluates hypergeometric distribution (1,1) cell table, given marginal totals. odds event occurs probability \\(p\\) given \\(p/(1-p)\\). Abbreviate \\(odds(p)\\) odds ratio two events probabilities \\(p_1\\) \\(p_2\\) \\(odds(p_1)/odds(p_2)\\). 2x2 table can estimated cross ratio “ad/bc”. odds ratio estimated fisher.test different, conditional maximum likelihood estimate. difference seldom meaningful.","code":"library(CSHstats) data(fos_ex) data(egr1_ex) fos_act = 1*(fos_ex > 1000) egr1_act = 1*(egr1_ex > 2000) acttab = table(fos_act=fos_act, egr1_act=egr1_act) acttab ##        egr1_act ## fos_act  0  1 ##       0 26  5 ##       1 25 23 fisher.test(acttab, alternative=\"greater\") # one-sided ##  ##  Fisher's Exact Test for Count Data ##  ## data:  acttab ## p-value = 0.0034 ## alternative hypothesis: true odds ratio is greater than 1 ## 95 percent confidence interval: ##  1.68701     Inf ## sample estimates: ## odds ratio  ##    4.69113 sum(dhyper(26:31, 51, 28, 31)) ## [1] 0.00339765 odds = function(x) {p=mean(x); p/(1-p)} odds(fos_act[egr1_act==TRUE])/odds(fos_act[egr1_act==FALSE]) ## [1] 4.784 acttab[1,1]*acttab[2,2]/(acttab[1,2]*acttab[2,1]) ## [1] 4.784"},{"path":"/articles/S5_linear_models.html","id":"binary-response-logistic-regression","dir":"Articles","previous_headings":"GLMs: binary, counted, non-Gaussian responses","what":"Binary response: logistic regression","title":"S5_inference: FDR, linear models, and GLMs","text":"model conditional probability FOS activated, depending whether note EGR1 activated, using definitions given . model form \\[ \\mbox{logit} Pr(FOS > 1000|EGR1>2000) = \\alpha + \\beta x \\] logit(p) = log(p/(1-p)) called “link function”. transforms outcome interest (probability) interval (0,1) whole real line. interpretation \\(\\beta\\) : “log odds ratio”. exponentiate beta, obtain effect change x one unit odds response event. Parameter interpretation. intercept parameter value -0.0392, logit probability FOS activated, given EGR1 activated. “beta” parameter log odds ratio (comparing probability FOS activation EGR1 conditions – activated activated).","code":"g1 = glm(fos_act ~ egr1_act, family=binomial()) summary(g1) ##  ## Call: ## glm(formula = fos_act ~ egr1_act, family = binomial()) ##  ## Coefficients: ##             Estimate Std. Error z value Pr(>|z|)    ## (Intercept)  -0.0392     0.2801   -0.14   0.8886    ## egr1_act      1.5653     0.5673    2.76   0.0058 ** ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ##  ## (Dispersion parameter for binomial family taken to be 1) ##  ##     Null deviance: 105.830  on 78  degrees of freedom ## Residual deviance:  96.958  on 77  degrees of freedom ## AIC: 101 ##  ## Number of Fisher Scoring iterations: 3 log(4.784) ## [1] 1.56528"},{"path":"/articles/S5_linear_models.html","id":"multiple-logistic-regression","dir":"Articles","previous_headings":"GLMs: binary, counted, non-Gaussian responses","what":"Multiple logistic regression","title":"S5_inference: FDR, linear models, and GLMs","text":"Fisher’s exact test simple logistic regression seen consistent one another example. advantage logistic regression framework potential sources variation probability response can incorporated. data YY1 expression ACC declare activated expressed level greater 1500. inclusion information YY1 seem informative FOS activation.","code":"data(yy1_ex) yy1_act = yy1_ex > 1500 g2 = glm(fos_act ~ egr1_act+yy1_act, family=binomial()) summary(g2) ##  ## Call: ## glm(formula = fos_act ~ egr1_act + yy1_act, family = binomial()) ##  ## Coefficients: ##             Estimate Std. Error z value Pr(>|z|)    ## (Intercept)   0.0922     0.3175    0.29   0.7716    ## egr1_act      1.5690     0.5702    2.75   0.0059 ** ## yy1_actTRUE  -0.4833     0.5472   -0.88   0.3771    ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ##  ## (Dispersion parameter for binomial family taken to be 1) ##  ##     Null deviance: 105.830  on 78  degrees of freedom ## Residual deviance:  96.177  on 76  degrees of freedom ## AIC: 102.2 ##  ## Number of Fisher Scoring iterations: 3"},{"path":"/articles/S5_linear_models.html","id":"general-case-link-and-variance-functions","dir":"Articles","previous_headings":"GLMs: binary, counted, non-Gaussian responses","what":"General case: link and variance functions","title":"S5_inference: FDR, linear models, and GLMs","text":"family generalized linear models can accommodate many forms variation response interest, associated number “predictor” variables. DESeq2 edgeR use negative binomial GLM inference differential expression RNA-seq. hear much . glm.nb function MASS package allows fit negative binomial regressions data data.frames.","code":"kable(glmtab)"},{"path":"/articles/S5_linear_models.html","id":"hierarchical-models-alternatives-to-independence","dir":"Articles","previous_headings":"","what":"Hierarchical models: alternatives to independence","title":"S5_inference: FDR, linear models, and GLMs","text":"strong assumption use linear generalized linear models mutual independence reponses. clustering responses present (example, analyzing data collected human families, pairs eyes, mice litters), additional model structure introduced accommodate lack independence.","code":""},{"path":"/articles/S5_linear_models.html","id":"random-effects-models-for-clustered-observations","dir":"Articles","previous_headings":"Hierarchical models: alternatives to independence","what":"Random effects models for clustered observations","title":"S5_inference: FDR, linear models, and GLMs","text":"framework approach developed 1982 (Laird Ware) expands linear model model known “mixed effects”: \\[ y_{ij} = \\alpha + \\beta x_{ij} + a_i + e_{ij} \\] \\(\\) indexing clusters \\(j\\) responses within clusters. “random effect” \\(a_i\\) assumed follow \\(N(0, \\sigma^2_b)\\) (-cluster variance), independent \\(e_{ij} \\sim N(0, \\sigma^2_w)\\). intraclass correlation coefficient measures departure independence, form \\(\\rho = \\sigma^2_b/(\\sigma^2_b + \\sigma^2_w)\\). Approximate mixed effects models GLM families can fit using glmmPQL MASS.","code":""},{"path":"/articles/S6_hca_onto.html","id":"road-map","dir":"Articles","previous_headings":"","what":"Road map","title":"S6 HCA, ontologies, shiny","text":"Bioconductor reads paper ontoProc rols help ontologies shiny helps communicate","code":""},{"path":"/articles/S6_hca_onto.html","id":"a-paper-on-the-atlas-of-the-prostate","dir":"Articles","previous_headings":"","what":"A paper on the atlas of the prostate","title":"S6 HCA, ontologies, shiny","text":"overview normals","code":""},{"path":[]},{"path":"/articles/S6_hca_onto.html","id":"surveying-projects-of-the-hca","dir":"Articles","previous_headings":"A paper on the atlas of the prostate > The hca package","what":"Surveying projects of the HCA","title":"S6 HCA, ontologies, shiny","text":"","code":"library(hca) p = projects(size = 200); p = dplyr::bind_rows(p, hca_next(p)) # workaround bug upstream to 1.4.0 library(DT) datatable(as.data.frame(p))"},{"path":"/articles/S6_hca_onto.html","id":"picking-a-project-enumerating-and-downloading-loom-files","dir":"Articles","previous_headings":"A paper on the atlas of the prostate > The hca package","what":"Picking a project; Enumerating and downloading loom files","title":"S6 HCA, ontologies, shiny","text":"","code":"projectId = \"53c53cd4-8127-4e12-bc7f-8fe1610a715c\" file_filter <- filters(     projectId = list(is = projectId),     fileFormat = list(is = \"loom\") ) pfile = files(file_filter) pfile$projectTitle[1] ## [1] \"A Cellular Anatomy of the Normal Adult Human Prostate and Prostatic Urethra\" #pfile |> files_download()"},{"path":"/articles/S6_hca_onto.html","id":"working-with-loom","dir":"Articles","previous_headings":"A paper on the atlas of the prostate > The hca package","what":"Working with loom","title":"S6 HCA, ontologies, shiny","text":"superficial filtering (60000 cells) development PCA Whatchaget:","code":"library(LoomExperiment) f1 = import(\"/home/stvjc/.cache/R/hca/36e582f7c6e_36e582f7c6e.loom\") f1 names(colData(f1)) library(scater) sf1 = as(f1, \"SingleCellExperiment\") sf1 library(scuttle) assay(sf1[1:4,1:4]) assayNames(sf1) = \"counts\" litsf1 = sf1[,1:60000] z = DelayedArray::rowSums(assay(litsf1)) mean(z==0) todrop = which(z==0) litsf2 = litsf1[-todrop,] assay(litsf2) litsf2 = logNormCounts(litsf2) litsf2 = runPCA(litsf2) library(SingleCellExperiment) if (!exists(\"litsf2\")) load(\"litsf2.rda\") # run code above, must have HDF5 in cache metadata(litsf2) ## $last_modified ## [1] \"20210616T032315.280827Z\" ##  ## $CreationDate ## [1] \"20210616T030548.106180Z\" ##  ## $LOOM_SPEC_VERSION ## [1] \"3.0.0\" ##  ## $expression_data_type ## [1] \"exonic\" ##  ## $input_id ## [1] \"5012cf4f-ba5c-4928-aea1-ab2891e34323\" ##  ## $input_id_metadata_field ## [1] \"sequencing_process.provenance.document_id\" ##  ## $input_name ## [1] \"D27PrTzF_Edn\" ##  ## $input_name_metadata_field ## [1] \"sequencing_input.biomaterial_core.biomaterial_id\" ##  ## $optimus_output_schema_version ## [1] \"1.0.0\" ##  ## $pipeline_version ## [1] \"Optimus_v4.2.3\" > str(litsf2) # 22MB on disk (no quantifications) Formal class 'DelayedMatrix' [package \"DelayedArray\"] with 1 slot   ..@ seed:Formal class 'DelayedAperm' [package \"DelayedArray\"] with 2 slots   .. .. ..@ perm: int [1:2] 2 1   .. .. ..@ seed:Formal class 'DelayedSubset' [package \"DelayedArray\"] with 2 slots   .. .. .. .. ..@ index:List of 2   .. .. .. .. .. ..$ : int [1:60000] 1 2 3 4 5 6 7 8 9 10 ...   .. .. .. .. .. ..$ : int [1:23420] 13 20 22 23 31 33 34 35 36 37 ...   .. .. .. .. ..@ seed :Formal class 'HDF5ArraySeed' [package \"HDF5Array\"] with 7 slots   .. .. .. .. .. .. ..@ filepath : chr \"/home/stvjc/.cache/R/hca/36e582f7c6e_36e582f7c6e.loom\"   .. .. .. .. .. .. ..@ name     : chr \"/matrix\"   .. .. .. .. .. .. ..@ as_sparse: logi FALSE   .. .. .. .. .. .. ..@ type     : chr NA   .. .. .. .. .. .. ..@ dim      : int [1:2] 382197 58347   .. .. .. .. .. .. ..@ chunkdim : int [1:2] 64 64   .. .. .. .. .. .. ..@ first_val: int 0 stvjc@stvjc-XPS-13-9300:~/CSAMA_HCA$ ls -tl /home/stvjc/.cache/R/hca/36e582f7c6e_36e582f7c6e.loom -rw-rw-r-- 1 stvjc stvjc 1206062245 Jun 21 22:37 /home/stvjc/.cache/R/hca/36e582f7c6e_36e582f7c6e.loom"},{"path":"/articles/S6_hca_onto.html","id":"working-with-isee","dir":"Articles","previous_headings":"A paper on the atlas of the prostate > The hca package","what":"Working with iSEE","title":"S6 HCA, ontologies, shiny","text":"Question: “stop/exit” button? Question: can embed iSEE (components) vignette? iSEE server? context fgf2","code":""},{"path":"/articles/S6_hca_onto.html","id":"upshots","dir":"Articles","previous_headings":"A paper on the atlas of the prostate","what":"Upshots","title":"S6 HCA, ontologies, shiny","text":"easy survey HCA hca package easy get experiments, metadata, quantifications projects interest iSEE really accelerates exploration elaboration data claims","code":""},{"path":[]},{"path":"/articles/S6_hca_onto.html","id":"definition-from-wikipedia","dir":"Articles","previous_headings":"Ontologies, EBI OLS, rols (thanks Laurent Gatto!), ontoProc::ctmarks","what":"Definition: from Wikipedia","title":"S6 HCA, ontologies, shiny","text":"computer science information science, ontology encompasses representation, formal naming, definition categories, properties, relations concepts, data, entities substantiate one, many, domains discourse. simply, ontology way showing properties subject area related, defining set concepts categories represent subject. Every academic discipline field creates ontologies limit complexity organize data information knowledge. uses ontological assumptions frame explicit theories, research applications. New ontologies may improve problem solving within domain. Translating research papers within every field problem made easier experts different countries maintain controlled vocabulary jargon languages.","code":""},{"path":"/articles/S6_hca_onto.html","id":"applications-in-genomics","dir":"Articles","previous_headings":"Ontologies, EBI OLS, rols (thanks Laurent Gatto!), ontoProc::ctmarks","what":"Applications in genomics","title":"S6 HCA, ontologies, shiny","text":"Gene Ontology: Genes gene products subdomains BP, MF, CC – biological process, molecular function, cellular component Human Phenotype Ontology UBERON - cross-species anatomy Cell ontology Cell line ontology EFO - experimental factor ontology Tags can encountered various annotation resources.","code":""},{"path":"/articles/S6_hca_onto.html","id":"rols-basic-idea","dir":"Articles","previous_headings":"Ontologies, EBI OLS, rols (thanks Laurent Gatto!), ontoProc::ctmarks","what":"rols: Basic idea","title":"S6 HCA, ontologies, shiny","text":"OLS ontology lookup service API rols package help interrogate service ontologies everywhere","code":""},{"path":"/articles/S6_hca_onto.html","id":"learn-about-smooth-muscle-with-rols","dir":"Articles","previous_headings":"Ontologies, EBI OLS, rols (thanks Laurent Gatto!), ontoProc::ctmarks","what":"Learn about ‘smooth muscle’ with rols","title":"S6 HCA, ontologies, shiny","text":"","code":"library(rols) ss = OlsSearch(\"smooth muscle\", rows=100) ss ## Object of class 'OlsSearch': ##   query: smooth muscle  ##   requested: 100 (out of 50306) ##   response(s): 0 tt = olsSearch(ss) dd = as(tt, \"data.frame\") datatable(dd)"},{"path":"/articles/S6_hca_onto.html","id":"ontoproc-capitalizing-on-ontologyindex-thanks-daniel-greene-rgraphviz-thanks-kasper-hansen","dir":"Articles","previous_headings":"Ontologies, EBI OLS, rols (thanks Laurent Gatto!), ontoProc::ctmarks","what":"ontoProc – capitalizing on ontologyIndex (thanks Daniel Greene!), Rgraphviz (thanks Kasper Hansen!)","title":"S6 HCA, ontologies, shiny","text":"ctmarks app: walk linked ontologies PR present additional facets concept focus Limitation: OBO representation use outdated -links sparse Projects: - use rols get interesting information terms app - update ontology resources - go beyond OBO … way OWL? Evaluate UI/UX needed broaden ontology usage - impacts: data integration, precision annotation, cognitive efficiency","code":"library(ontoProc) co = getOnto(\"cellOnto\") ## loading from cache head(co$name) ##              BFO:0000002              BFO:0000003              BFO:0000004  ##             \"continuant\"              \"occurrent\" \"independent continuant\"  ##              BFO:0000006              BFO:0000015              BFO:0000016  ##         \"spatial region\"                \"process\"            \"disposition\" chk = ctmarks(co)"},{"path":[]},{"path":"/articles/distplus.html","id":null,"dir":"Articles","previous_headings":"","what":"Road map","title":"distances, nearest-neighbor graphs, clustering","text":"Distances high-dimensional spaces Criteria agglomerative clustering interactive heatmap comparing Euclidean correlation (1-cor) distances comparing agglomeration methods silhouette measure clustering adequacy bluster tools cluster diagnostics Grun pancreas data NNGraphParam","code":""},{"path":"/articles/distplus.html","id":null,"dir":"Articles","previous_headings":"","what":"Cluster analysis concepts","title":"distances, nearest-neighbor graphs, clustering","text":"need identify groups similar observations arises many contexts – ultimately service clarifying sources variability, building power statistical comparisons. Hierarchical clustering N multivariate observations conducted starting N clusters proceeding M clusters M-1 (M=N, …, 2) clusters merging two members separated least pairwise distances.","code":""},{"path":[]},{"path":[]},{"path":"/articles/distplus.html","id":null,"dir":"Articles","previous_headings":"","what":"Application: inferring steps in tumor metastasis in a breast cancer patient","title":"distances, nearest-neighbor graphs, clustering","text":"’ll examine data distributed 2021 Genome Biology paper Gabor Marth lab. Clinical sequence interventions. Event sequence.","code":""},{"path":"/articles/distplus.html","id":null,"dir":"Articles","previous_headings":"","what":"A view of copy number aberrations for 1Mb tiling","title":"distances, nearest-neighbor graphs, clustering","text":"28 tumors sampled sequenced rapid autopsy procedure. Copy number variation assessed using FACETS. tissues tumors taken Br (Breast), Bo (Bone), Bn (Brain), Ln (Lung), Lv (Liver), Pa (Pancreas), Ly (Lymph nodes), Kd (Kidney) plotly-based visualization (vertical) ordering tissues chosen exemplify certain similarities. example block blue chr10 seen three samples. indication deletion.","code":""},{"path":"/articles/distplus.html","id":null,"dir":"Articles","previous_headings":"","what":"A cluster analysis proposed in support of the evolutionary map","title":"distances, nearest-neighbor graphs, clustering","text":"code lightly modified script distributed https://github.com/xiaomengh/tumor-evo-rapid-autopsy.git.","code":"suppressPackageStartupMessages({  library(csamaDist)  library(bioDist)  library(bluster)  library(cluster)  library(scater)  library(scran)  library(scRNAseq)  library(scuttle) }) data(cnv_log_R) data = cnv_log_R samples = c('Ln7','Ln9','Ln1','BrM','BrP',            'Ln11','Ly2','Ln3',            'Bo3','Ln10','Bo1','Ln8','Lv3','Ln5','Bo2','Bn2','Bn1','Bn3','Bn4','Ln2',            'Ly1','Ln6',            'Kd1','Ln4','Lv4','Lv2','Lv1','Pa1') rownames(data) = samples d = dist(data, method=\"euclidean\") fit = hclust(d, method=\"average\") # the following line changes the order of the samples to produce the Fig.S3B but doesn't change the phylogenetic relationship fit$order = c(1,4,2,5,3,13,10,20,16,11,12,15,9,17,19,14,18,21,22,7,6,8,25,27,26,28,23,24) plot(fit)"},{"path":[]},{"path":"/articles/distplus.html","id":null,"dir":"Articles","previous_headings":"Drilling down on the clustering","what":"Comparing Euclidean and Correlation distances","title":"distances, nearest-neighbor graphs, clustering","text":"given correlation distance value, can wide variation euclidean distance, vice versa. Open question: distance metric relevant biological interpretation CNV?","code":"cd = cor.dist(cnv_log_R) # from bioDist ed = dist(cnv_log_R) plot(as.numeric(ed), as.numeric(cd), xlab=\"All pairwise Euclidean distances\", ylab=\"All pairwise correlation distances\")"},{"path":"/articles/distplus.html","id":null,"dir":"Articles","previous_headings":"Drilling down on the clustering","what":"A pair with discrepant correlation and euclidean distance values (over entire genome)","title":"distances, nearest-neighbor graphs, clustering","text":"’ll look first 100Mb chr1.","code":"plot(cnv_log_R[\"Ly1\",1:100],pch=19, main=\"chr1, first 100Mb\", ylab=\"FACETS CNV log R\", xlab=\"chr1\") points(cnv_log_R[\"Ln1\",1:100], col=\"red\",pch=19) legend(60, -.5, pch=19, col=c(\"black\", \"red\"), legend=c(\"Ly1\", \"Ln1\")) #cor(cnv_log_R[\"Ly1\", 1:100], cnv_log_R[\"Ln1\", 1:100]) edist = function(x,y) sqrt(sum((x-y)^2)) edist(cnv_log_R[\"Ly1\", 1:100], cnv_log_R[\"Ln1\", 1:100]) ## [1] 1.685992 plot(jitter(cnv_log_R[\"Ly1\", 1:100]), cnv_log_R[\"Ln1\", 1:100], xlab=\"Ly1\", ylab=\"Ln1\") abline(0,1)"},{"path":"/articles/distplus.html","id":null,"dir":"Articles","previous_headings":"Drilling down on the clustering","what":"Redo clustering with alternative distance and agglomeration method","title":"distances, nearest-neighbor graphs, clustering","text":"","code":"fit2 = hclust(cd, method=\"ward.D2\") plot(fit2) abline(h=.3, lty=2)"},{"path":"/articles/distplus.html","id":null,"dir":"Articles","previous_headings":"Drilling down on the clustering","what":"Silhouette measure","title":"distances, nearest-neighbor graphs, clustering","text":"?silhouette cluster library:","code":"For each observation i, the _silhouette width_ s(i) is defined as follows:       Put a(i) = average dissimilarity between i and all other points of      the cluster to which i belongs (if i is the _only_ observation in      its cluster, s(i) := 0 without further calculations).  For all      _other_ clusters C, put d(i,C) = average dissimilarity of i to all      observations of C.  The smallest of these d(i,C) is b(i) := \\min_C      d(i,C), and can be seen as the dissimilarity between i and its      \"neighbor\" cluster, i.e., the nearest one to which it does _not_      belong.  Finally,                     s(i) := ( b(i) - a(i) ) / max( a(i), b(i) ).                     'silhouette.default()' is now based on C code donated by Romain      Francois (the R version being still available as      'cluster:::silhouette.default.R').       Observations with a large s(i) (almost 1) are very well clustered,      a small s(i) (around 0) means that the observation lies between      two clusters, and observations with a negative s(i) are probably      placed in the wrong cluster. ct1 = cutree(fit2, h=.3) c2 = cnv_log_R rownames(c2) = paste(rownames(c2), as.numeric(ct1)) sil = silhouette(ct1, cd) plot(sil)"},{"path":"/articles/distplus.html","id":null,"dir":"Articles","previous_headings":"Drilling down on the clustering","what":"Exercises","title":"distances, nearest-neighbor graphs, clustering","text":"1: install bioDist vjcitn/csamaDist. Use code: Comment qualitative differences clusterings. 2: produce report silhouette measurement second clustering. Produce three-cluster partition hc1 obtain silhouette display.","code":"library(csamaDist) data(cnv_log_R) hc1 = hclust(dist(cnv_log_R[1:8,])) hc2 = hclust(bioDist::cor.dist(cnv_log_R[1:8,]), method=\"ward.D2\")  opar = par(no.readonly=TRUE) par(mfrow=c(1,2), mar=c(4,3,1,1)) plot(hc1, main=\"Euc, complete\") plot(hc2, main=\"1-Cor, Ward's D2\") par(opar) assn =  cutree(hc2, h=.25) plot(silhouette( assn, bioDist::cor.dist(cnv_log_R[1:8,])))"},{"path":"/articles/distplus.html","id":null,"dir":"Articles","previous_headings":"","what":"Clustering single cell RNA-seq","title":"distances, nearest-neighbor graphs, clustering","text":"code taken verbatim bluster “diagnostics” vignette.","code":""},{"path":"/articles/distplus.html","id":null,"dir":"Articles","previous_headings":"","what":"Acquire Grun et al’s single cell RNA-seq dataset","title":"distances, nearest-neighbor graphs, clustering","text":"[Grun 2016] (https://www.sciencedirect.com/science/article/pii/S1934590916300947) define algorithm, StemID, infers candidate multipotent cell populations human pancreas.","code":"library(scRNAseq) sce <- GrunPancreasData() ## snapshotDate(): 2022-04-26 ## see ?scRNAseq and browseVignettes('scRNAseq') for documentation ## loading from cache ## snapshotDate(): 2022-04-26 ## see ?scRNAseq and browseVignettes('scRNAseq') for documentation ## loading from cache # Quality control to remove bad cells. library(scuttle) qcstats <- perCellQCMetrics(sce) qcfilter <- quickPerCellQC(qcstats, sub.fields=\"altexps_ERCC_percent\") ## Warning in .get_med_and_mad(metric, batch = batch, subset = subset, ## share.medians = share.medians, : missing values ignored during outlier detection sce <- sce[,!qcfilter$discard]  # Normalization by library size. sce <- logNormCounts(sce)  # Feature selection. library(scran) dec <- modelGeneVar(sce) hvgs <- getTopHVGs(dec, n=1000)  # Dimensionality reduction. set.seed(1000) library(scater) sce <- runPCA(sce, ncomponents=20, subset_row=hvgs) sce <- runTSNE(sce, subset_row=hvgs)"},{"path":"/articles/distplus.html","id":null,"dir":"Articles","previous_headings":"","what":"Clustering using a nearest-neighbor graph; visualization via TSNE and PCA","title":"distances, nearest-neighbor graphs, clustering","text":"bluster’s makeSNNGraph help page","code":"The 'makeSNNGraph' function builds a shared nearest-neighbour    graph using observations as nodes. For each observation, its 'k'    nearest neighbours are identified using the 'findKNN' function,    based on distances between their expression profiles (Euclidean by    default). An edge is drawn between all pairs of observations that    share at least one neighbour, weighted by the characteristics of    the shared nearest neighbors - see \"Weighting Schemes\" below.     The aim is to use the SNN graph to perform clustering of    observations via community detection algorithms in the 'igraph'    package. This is faster and more memory efficient than    hierarchical clustering for large numbers of observations. In    particular, it avoids the need to construct a distance matrix for    all pairs of observations. Only the identities of nearest    neighbours are required, which can be obtained quickly with    methods in the 'BiocNeighbors' package. library(bluster) mat <- reducedDim(sce) clust.info <- clusterRows(mat, NNGraphParam(), full=TRUE) clusters <- clust.info$clusters table(clusters) ## clusters ##   1   2   3   4   5   6   7   8   9  10  11  12  ## 285 171 161  59 174  49  70 137  69  65  28  23 plot(reducedDims(sce)$TSNE, col=clusters, pch=19, main=\"TSNE\") plot(reducedDims(sce)$PCA, col=clusters, pch=19, main=\"PCA\") pairs(reducedDims(sce)$PCA[,1:4], col=clusters, pch=19, main=\"PCA\")"},{"path":"/articles/distplus.html","id":null,"dir":"Articles","previous_headings":"","what":"Assessment via pairwiseModularity","title":"distances, nearest-neighbor graphs, clustering","text":"","code":"g <- clust.info$objects$graph ratio <- pairwiseModularity(g, clusters, as.ratio=TRUE)  cluster.gr <- igraph::graph_from_adjacency_matrix(log2(ratio+1),      mode=\"upper\", weighted=TRUE, diag=FALSE)  # Increasing the weight to increase the visibility of the lines. set.seed(1100101) plot(cluster.gr, edge.width=igraph::E(cluster.gr)$weight*5,     layout=igraph::layout_with_lgl)"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Vince Carey. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Carey V (2023). CSHstats: discussions exercises classical statistics CSHL 2022. R package version 0.0.26, https://github.com/vjcitn/CSHstats.","code":"@Manual{,   title = {CSHstats: discussions and exercises on classical statistics for CSHL 2022},   author = {Vince Carey},   year = {2023},   note = {R package version 0.0.26},   url = {https://github.com/vjcitn/CSHstats}, }"},{"path":"/index.html","id":"cshstats","dir":"","previous_headings":"","what":"discussions and exercises on classical statistics for CSHL 2022","title":"discussions and exercises on classical statistics for CSHL 2022","text":"R vignettes/pkgdown elementary statistics summer course DRAFT! preliminary 19 May 2022 One view conceptual scheme course three main vertices:  students start lower left vertex need move along bottom edge work high-dimensional data generated sequencing technologies. Ultimately want use data climb edges expand biological medical knowledge. ’ll conduct introductory exploration four topic areas statistics data science: Probability simulation Exploratory data analysis visualization Hypothesis testing Linear models MultiAssayExperiment applications SingleCellMultimodal (convenience) CITEseq TENx MultiOme seqFISH","code":""},{"path":"/reference/accex.html","id":null,"dir":"Reference","previous_headings":"","what":"MultiAssayExperiment instance for ACC — accex","title":"MultiAssayExperiment instance for ACC — accex","text":"MultiAssayExperiment instance ACC","code":""},{"path":"/reference/accex.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MultiAssayExperiment instance for ACC — accex","text":"","code":"accex"},{"path":"/reference/accex.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"MultiAssayExperiment instance for ACC — accex","text":"MultiAssayExperiment","code":""},{"path":"/reference/build_deck.html","id":null,"dir":"Reference","previous_headings":"","what":"make a deck of cards with 52 elements and unicode symbols for suits — build_deck","title":"make a deck of cards with 52 elements and unicode symbols for suits — build_deck","text":"make deck cards 52 elements unicode symbols suits","code":""},{"path":"/reference/build_deck.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"make a deck of cards with 52 elements and unicode symbols for suits — build_deck","text":"","code":"build_deck()"},{"path":"/reference/c50.html","id":null,"dir":"Reference","previous_headings":"","what":"meaningless integers — c50","title":"meaningless integers — c50","text":"meaningless integers","code":""},{"path":"/reference/c50.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"meaningless integers — c50","text":"","code":"c50"},{"path":"/reference/c50.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"meaningless integers — c50","text":"numeric vector","code":""},{"path":"/reference/c5000.html","id":null,"dir":"Reference","previous_headings":"","what":"read depth for 5000 positions on pasilla chr4, from example(coverage) for GenomicAlignments — c5000","title":"read depth for 5000 positions on pasilla chr4, from example(coverage) for GenomicAlignments — c5000","text":"read depth 5000 positions pasilla chr4, example(coverage) GenomicAlignments","code":""},{"path":"/reference/c5000.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"read depth for 5000 positions on pasilla chr4, from example(coverage) for GenomicAlignments — c5000","text":"","code":"c5000"},{"path":"/reference/c5000.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"read depth for 5000 positions on pasilla chr4, from example(coverage) for GenomicAlignments — c5000","text":"numeric vector","code":""},{"path":"/reference/doubsim.html","id":null,"dir":"Reference","previous_headings":"","what":"simulated draws of top card from a shuffled deck of cards — doubsim","title":"simulated draws of top card from a shuffled deck of cards — doubsim","text":"simulated draws top card shuffled deck cards","code":""},{"path":"/reference/doubsim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"simulated draws of top card from a shuffled deck of cards — doubsim","text":"","code":"doubsim"},{"path":"/reference/doubsim.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"simulated draws of top card from a shuffled deck of cards — doubsim","text":"character vector","code":""},{"path":"/reference/doubsim2.html","id":null,"dir":"Reference","previous_headings":"","what":"simulated draws of top card from a shuffled  deck of cards — doubsim2","title":"simulated draws of top card from a shuffled  deck of cards — doubsim2","text":"simulated draws top card shuffled  deck cards","code":""},{"path":"/reference/doubsim2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"simulated draws of top card from a shuffled  deck of cards — doubsim2","text":"","code":"doubsim2"},{"path":"/reference/doubsim2.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"simulated draws of top card from a shuffled  deck of cards — doubsim2","text":"character vector","code":""},{"path":"/reference/doubsim3.html","id":null,"dir":"Reference","previous_headings":"","what":"simulated draws of top card from a shuffled  deck of cards — doubsim3","title":"simulated draws of top card from a shuffled  deck of cards — doubsim3","text":"simulated draws top card shuffled  deck cards","code":""},{"path":"/reference/doubsim3.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"simulated draws of top card from a shuffled  deck of cards — doubsim3","text":"","code":"doubsim3"},{"path":"/reference/doubsim3.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"simulated draws of top card from a shuffled  deck of cards — doubsim3","text":"character vector","code":""},{"path":"/reference/egr1_ex.html","id":null,"dir":"Reference","previous_headings":"","what":"RNASeq2GeneNorm measures for EGR1 from TCGA ACC samples — egr1_ex","title":"RNASeq2GeneNorm measures for EGR1 from TCGA ACC samples — egr1_ex","text":"RNASeq2GeneNorm measures EGR1 TCGA ACC samples","code":""},{"path":"/reference/egr1_ex.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"RNASeq2GeneNorm measures for EGR1 from TCGA ACC samples — egr1_ex","text":"","code":"egr1_ex"},{"path":"/reference/egr1_ex.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"RNASeq2GeneNorm measures for EGR1 from TCGA ACC samples — egr1_ex","text":"numeric vector","code":""},{"path":"/reference/faces.html","id":null,"dir":"Reference","previous_headings":"","what":"get faces of a collection of cards — faces","title":"get faces of a collection of cards — faces","text":"get faces collection cards","code":""},{"path":"/reference/faces.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"get faces of a collection of cards — faces","text":"","code":"faces(x)"},{"path":"/reference/faces.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"get faces of a collection of cards — faces","text":"x card deck made `build_deck`","code":""},{"path":"/reference/faces.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"get faces of a collection of cards — faces","text":"","code":"d = build_deck() d[1] #> [1] \"2 ♡\" faces(d[1:2]) #> [1] \"2\" \"3\""},{"path":"/reference/fos_ex.html","id":null,"dir":"Reference","previous_headings":"","what":"RNASeq2GeneNorm measures for FOS from TCGA ACC samples — fos_ex","title":"RNASeq2GeneNorm measures for FOS from TCGA ACC samples — fos_ex","text":"RNASeq2GeneNorm measures FOS TCGA ACC samples","code":""},{"path":"/reference/fos_ex.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"RNASeq2GeneNorm measures for FOS from TCGA ACC samples — fos_ex","text":"","code":"fos_ex"},{"path":"/reference/fos_ex.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"RNASeq2GeneNorm measures for FOS from TCGA ACC samples — fos_ex","text":"numeric vector","code":""},{"path":"/reference/full_house.html","id":null,"dir":"Reference","previous_headings":"","what":"test for full house — full_house","title":"test for full house — full_house","text":"test full house","code":""},{"path":"/reference/full_house.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"test for full house — full_house","text":"","code":"full_house(x)"},{"path":"/reference/full_house.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"test for full house — full_house","text":"x vector cards","code":""},{"path":"/reference/full_house.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"test for full house — full_house","text":"","code":"d = build_deck() h1 = c(d[1], d[14], d[27], d[2], d[15]) h1 #> [1] \"2 ♡\" \"2 ♢\" \"2 ♣\" \"3 ♡\" \"3 ♢\" full_house(h1) #> [1] TRUE h2 = c(d[1], d[14], d[27], d[2], d[16]) h2 #> [1] \"2 ♡\" \"2 ♢\" \"2 ♣\" \"3 ♡\" \"4 ♢\" full_house(h2) #> [1] FALSE"},{"path":"/reference/gtex_exc_chr20_b38.html","id":null,"dir":"Reference","previous_headings":"","what":"data.frame with GTEx lung eQTL data and metadata, a selection from chr20 — gtex_exc_chr20_b38","title":"data.frame with GTEx lung eQTL data and metadata, a selection from chr20 — gtex_exc_chr20_b38","text":"data.frame GTEx lung eQTL data metadata, selection chr20","code":""},{"path":"/reference/gtex_exc_chr20_b38.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"data.frame with GTEx lung eQTL data and metadata, a selection from chr20 — gtex_exc_chr20_b38","text":"","code":"gtex_exc_chr20_b38"},{"path":"/reference/gtex_exc_chr20_b38.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"data.frame with GTEx lung eQTL data and metadata, a selection from chr20 — gtex_exc_chr20_b38","text":"data.frame","code":""},{"path":"/reference/is_flush.html","id":null,"dir":"Reference","previous_headings":"","what":"test for flush — is_flush","title":"test for flush — is_flush","text":"test flush","code":""},{"path":"/reference/is_flush.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"test for flush — is_flush","text":"","code":"is_flush(x)"},{"path":"/reference/is_flush.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"test for flush — is_flush","text":"x vector cards","code":""},{"path":"/reference/is_flush.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"test for flush — is_flush","text":"","code":"d = build_deck() h1 = c(d[1:4], d[6]) h1 #> [1] \"2 ♡\" \"3 ♡\" \"4 ♡\" \"5 ♡\" \"7 ♡\" is_flush(h1) #> [1] TRUE"},{"path":"/reference/one_pair.html","id":null,"dir":"Reference","previous_headings":"","what":"test for one pair — one_pair","title":"test for one pair — one_pair","text":"test one pair","code":""},{"path":"/reference/one_pair.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"test for one pair — one_pair","text":"","code":"one_pair(x)"},{"path":"/reference/one_pair.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"test for one pair — one_pair","text":"x vector cards","code":""},{"path":"/reference/one_pair.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"test for one pair — one_pair","text":"logical(1)","code":""},{"path":"/reference/one_pair.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"test for one pair — one_pair","text":"","code":"d = build_deck() hand_1 = c(d[1], d[14], d[2:4] ) hand_1 #> [1] \"2 ♡\" \"2 ♢\" \"3 ♡\" \"4 ♡\" \"5 ♡\" one_pair(hand_1[1]) #> [1] FALSE hand_2 = c(d[1], d[6], d[2:4] ) hand_2 #> [1] \"2 ♡\" \"7 ♡\" \"3 ♡\" \"4 ♡\" \"5 ♡\" one_pair(hand_2[2]) #> [1] FALSE"},{"path":"/reference/queryCHCA.html","id":null,"dir":"Reference","previous_headings":"","what":"use a snapshot of CuratedQueryAtlasR through duckdb — queryCHCA","title":"use a snapshot of CuratedQueryAtlasR through duckdb — queryCHCA","text":"use snapshot CuratedQueryAtlasR duckdb","code":""},{"path":"/reference/queryCHCA.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"use a snapshot of CuratedQueryAtlasR through duckdb — queryCHCA","text":"","code":"queryCHCA(con, inst = FALSE)"},{"path":"/reference/queryCHCA.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"use a snapshot of CuratedQueryAtlasR through duckdb — queryCHCA","text":"con connection duckdb::duckdb(...) inst logical(1) TRUE install httpfs duckdb","code":""},{"path":"/reference/queryCHCA.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"use a snapshot of CuratedQueryAtlasR through duckdb — queryCHCA","text":"dplyr::tbl metadata","code":""},{"path":"/reference/queryCHCA.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"use a snapshot of CuratedQueryAtlasR through duckdb — queryCHCA","text":"","code":"if (interactive()) { con = DBI::dbConnect(duckdb::duckdb()) queryCHCA(con) |> select(sample_) |> head() DBI::dbDisconnect(con) }"},{"path":"/reference/suits.html","id":null,"dir":"Reference","previous_headings":"","what":"get suits of a collection of cards — suits","title":"get suits of a collection of cards — suits","text":"get suits collection cards","code":""},{"path":"/reference/suits.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"get suits of a collection of cards — suits","text":"","code":"suits(x)"},{"path":"/reference/suits.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"get suits of a collection of cards — suits","text":"x card deck made `build_deck`","code":""},{"path":"/reference/suits.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"get suits of a collection of cards — suits","text":"","code":"d = build_deck() d[1] #> [1] \"2 ♡\" suits(d[1:2]) #> [1] \"♡\" \"♡\""},{"path":"/reference/two_pairs.html","id":null,"dir":"Reference","previous_headings":"","what":"test for two pairs — two_pairs","title":"test for two pairs — two_pairs","text":"test two pairs","code":""},{"path":"/reference/two_pairs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"test for two pairs — two_pairs","text":"","code":"two_pairs(x)"},{"path":"/reference/two_pairs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"test for two pairs — two_pairs","text":"x vector cards","code":""},{"path":"/reference/two_pairs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"test for two pairs — two_pairs","text":"","code":"d = build_deck() h1 = c(d[1], d[14], d[27], d[2], d[15]) h1 #> [1] \"2 ♡\" \"2 ♢\" \"2 ♣\" \"3 ♡\" \"3 ♢\" two_pairs(h1) #> [1] FALSE"},{"path":"/reference/yy1_ex.html","id":null,"dir":"Reference","previous_headings":"","what":"RNASeq2GeneNorm measures for YY1 from TCGA ACC samples — yy1_ex","title":"RNASeq2GeneNorm measures for YY1 from TCGA ACC samples — yy1_ex","text":"RNASeq2GeneNorm measures YY1 TCGA ACC samples","code":""},{"path":"/reference/yy1_ex.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"RNASeq2GeneNorm measures for YY1 from TCGA ACC samples — yy1_ex","text":"","code":"yy1_ex"},{"path":"/reference/yy1_ex.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"RNASeq2GeneNorm measures for YY1 from TCGA ACC samples — yy1_ex","text":"numeric vector","code":""}]
